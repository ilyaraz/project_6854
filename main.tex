\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{ccfonts}
\usepackage[T1]{fontenc}

\renewcommand{\bfdefault}{sbc}

\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\eps}{\varepsilon}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\XComment}[1]{}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}

\begin{document}
    \title{Spectral Sparsification of Undirected Graphs}
    \author{Ilya Razenshteyn\\\texttt{ilyaraz@mit.edu}}
    \maketitle

    \section{Introduction}

    One recurring topic in Graph Theory is how to approximate a given graph with much sparser one while preserving certain
    properties. A very useful and clean
    notion of \emph{cut sparsification} was introduced in~\cite{BK96} by Bencz\'{u}r and Karger.
    They proved the following result.

    \begin{theorem}[\cite{BK96}]
        \label{benczur_karger}
        Let $\eps > 0$ be some sufficiently small parameter.
        For every undirected weighted graph $G$ there exists another undirected weighted graph $\tilde{G}$ with the
        same set of vertices and only $O(n \log n / \eps^2)$ edges such that every cut value in $\tilde{G}$
        is within $(1 \pm \eps)$ of the corresponding value in $G$. Moreover, $\tilde{G}$ can be found in time
        $O(m \log^3 n)$ with high probability.
    \end{theorem}

    Two obvious applications of Theorem~\ref{benczur_karger} are the following:
    \begin{itemize}
        \item by sparsifying graph and then running the algorithm of Goldberg and Rao~\cite{GR98} we can find
        $(1 + \eps)$-approximations to the maximum $(s, t)$-flow and the minimum $(s, t)$-cut in time
        $O(m \log^3 n + n^{3/2} \log n \log(nU) / \eps^3)$, where $U$ is an upper bound on the capacities in our network;
        \item by sparsifying graph and then running the algorithm of Klein, Stein and Tardos~\cite{KST90} we can find
        an $O(\log n)$-approximation to the sparsest cut in time $O((m + n^2) \log^3 n)$.
    \end{itemize}

    The proof of Theorem~\ref{benczur_karger} goes along the following lines. The first naive idea is to employ uniform
    random sampling (in case of unweighted graphs).
    It turns out that one can prove that it works, provided that the minimum cut is large enough.
    To overcome this problem with small cuts one has to sample edges with probabilities proportional to their
    \emph{strong connectivities}.

    In~\cite{ST11} Spielman and Teng considered a singificantly stronger notion of sparsification: the so called
    \emph{spectral sparsification}. To define it we need a notion of \emph{graph Laplacian}.

    \begin{definition}
        Let $G = (V, E, w)$ be an undirected weighted graph. Then the following quadratic form is called \emph{Laplacian}
        of $G$:
        $$
            \mathcal{L}_G(x) = \sum_{(u, v) \in E} w(u, v) (x_u - x_v)^2.
        $$
    \end{definition}
    \begin{definition}
        We say that $\tilde{G}$ is a spectral sparsifier of $G$, if for every $x \in \Rbb^n$
        $$
            \mathcal{L}_{\tilde{G}}(x) \in (1 \pm \eps) \mathcal{L}_G(x).
        $$
    \end{definition}

    To see why the notion of spectral sparsification is stronger than that of cut sparsification it is sufficient to observe
    that $\mathcal{L}_G(\mathbf{1}_S)$ is exactly equal to the cut value of $(S, \bar{S})$.
    But we require much more: to preserve $\mathcal{L}_G(x)$ for every $x$, not only for the indicators.

    We will survey two constructions of spectral sparsifiers. The first is from~\cite{SS11}, which gives sparsifiers
    with $O(n \log n / \eps^2)$ edges
    matching the bound of Theorem~\ref{benczur_karger}. Moreover, these sparsifiers can be found
    in near-linear time. The second construction is from~\cite{BSS09}: it gives sparsifiers with $O(n / \eps^2)$ edges,
    but the algorithm for building them is much slower (nevertheless, even the existance of near-linear-sized spectral
    sparsifiers is higly non-trivial).

    Let us mention several applications of spectral sparsification.

    First, since spectral sparsifiers approximately preserve spectra of Laplacians, one can look at sparsifiers of the
    complete graph $K_n$ as expander graphs~\cite{HLW06}. Technically speaking, this analogy is quite vague, because
    expanders are constant-degree regular graphs, on the other hand, even the best construction of spectral sparsifiers
    from~\cite{BSS09}, despite giving a constant \emph{average} degree, does not provide graphs with constant maximum
    degree. Nevetheless, it is proved in~\cite{BSS09} that spectral sparsifiers of $K_n$ share several properties with
    constant-degree expanders.

    Second, spectral sparsification can be used to prove the existence of good \emph{approximate John decompositions}
    (see~\cite{B97} for an excellent introduction to Convex Geometry and~\cite{N11} for a great exposition of geometric
    applications of~\cite{BSS09}).

    Third, spectral sparsification can be used for dimension reduction in $\ell_1$-spaces.
    The classic theorem of Johnson--Lindenstrauss~\cite{DG03}
    says that if we have $n$ vectors in $\mathbb{R}^d$, then we can map them into $\mathbb{R}^{O(\log n / \eps^2)}$
    so that to $(1\pm\eps)$-preserve their pairwise $\ell_2$-distances.
    This theorem has immense number of applications (in particular, we will use it to find spectral sparsifiers quickly).
    It would be great to extend it to $\ell_1$, but in the series of papers~\cite{BC05}, \cite{LN04}, \cite{ACNN11},
    \cite{R11a} it was proved that one needs dimension to be essentially linear in $n$ in this case.
    Nevertheless, in~\cite{T90}
    it was proved that one can reduce dimension to $O(n \log n / \eps^2)$. Using spectral sparsification one can sharpen
    this bound to $O(n / \eps^2)$ (see~\cite{N11} for the exposition).

    \section{Spectral Sparsification}

    \subsection{Preliminaries}

    \subsubsection{Linear Algebra 101}

    Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix. 
    \begin{theorem}[Eigendecomposition]
        All the eigenvalues
        of $A$ are real and there exists an orthonormal basis of $\mathbb{R}^n$
        that consists of $A$'s eigenvectors.

        We can write
        $$
            A = \sum_{i=1}^n \lambda_i u_i u_i^t,
        $$
        where $\lambda_1, \lambda_2, \ldots, \lambda_n$ are the eigenvalues of $A$, and
        $u_1, u_2, \ldots, u_n$ are the corresponding eigenvectors that form an orthonormal
        basis of $\mathbb{R}^n$. This decomposition is called \emph{eigendecomposition}
        of $A$.
    \end{theorem}

    We can use the notion of eigendecomposition to introduce a pseudoinverse of $A$.

    \begin{definition}[Pseudoinverse]
        A \emph{pseudoinverse} of $A$ is the following matrix:
        $$
            A^+ := \sum_{i : \lambda_i \ne 0} \frac{1}{\lambda_i} u_i u_i^t.
        $$
    \end{definition}

    Clearly, if $A$ is non-degenerate (equivalently, $\lambda_i \ne 0$ for every $i$),
    then $A^+ = A^{-1}$.
    On the other hand, for every symmetric $A$
    $$
        A A^+ = A^+ A = I_{\mathrm{im}(A)},
    $$
    where $I_{\mathrm{im}(A)}$ is a linear operator that maps vectors from $\mathrm{ker}(A)$
    to zero and maps vectors from $\mathrm{im}(A)$ to themselves (this definition is correct,
    because we can consider a decomposition
    $\mathbb{R}^n = \mathrm{ker}(A) \oplus \mathrm{im}(A)$).

    If a symmetric matrix is positive semi-definite (equivalently, all the eigenvalues
    are non-negative), then we can consider its square root.

    \begin{definition}[Square Root of a PSD Matrix]
        $$
            A^{1/2} := \sum_{i} \sqrt{\lambda_i} u_i u_i^t.
        $$
    \end{definition}
    Clearly, $A^{1/2} A^{1/2} = A$.

    During the presentation of spectral sparsification algorithms we will use the following
    matrix norm crucially.

    \begin{definition}[Spectral Norm]
        $$
            \|A\|_2 := \max_{x} \frac{\|Ax\|_2}{\|x\|_2}.
        $$
    \end{definition}

    For symmetric matrices spectral norm is just the maximum absolute value of an eigenvalue.
    If $I$ is the identity matrix, then $\|A - I\|_2 \leq \eps$ iff all the eigenvalues of
    $A$ lie within $[1 - \eps; 1 + \eps]$.

    \subsubsection{Graph Laplacian}

    Let $G = (V, E)$ be an undirected graph.

    \begin{definition}[Laplacian]
        The following $n \times n$ matrix $L$ is called the Laplacian of $G$:
        $$
            x^t L x = \sum_{(v,w) \in E} (x_v - x_w)^2. 
        $$
    \end{definition}

    Clearly, $L_{vv}$ equals to the degree of $v$ and $L_{vw} = -1$, if $(v, w) \in E$,
    and $0$, otherwise.
    Since $L$ is positive-semidefinite, all $L$'s eigenvalues are non-negative. Let
    $0 \leq \lambda_1 \leq \lambda_2 \leq \ldots \leq \lambda_n$ be the eigenvalues of $L$.

    \begin{lemma}
        For every graph $G$ $\lambda_1 = 0$. Moreover, $G$ is connected iff $\lambda_2 > 0$.
    \end{lemma}
    \begin{proof}
        Clearly, $\mathbf{1}^t L \mathbf{1} = 0$, so, $\lambda_1 = 0$ and the corresponding
        eigenvector is $\mathbf{1} / \sqrt{n}$.

        If there is a non-zero
        vector within the kernel of $L$ that is orthogonal to $\mathbf{1}$
        it is easy to see that there are no edges connecting negative and positive
        coordinates, so $G$ is disconnected. Conversely, if $G$ is connected and
        $x^t L x = 0$, then, clearly, $x = \alpha \mathbf{1}$.
    \end{proof}

    So, if $G$ is connected, then $\mathrm{ker}(L) = \langle \mathbf{1} \rangle$
    and $\mathrm{im}(L) = \langle \mathbf{1} \rangle^{\perp}$.

    It turns out that $\lambda_2$ determines how well-connected $G$ is. The larger
    $\lambda_2$ is, the better connected $G$ is, and vice versa. This fact is known
    as Cheeger's inequality.

    We can decompose $L$ as follows:
    $$
        L = \sum_{(v,w) \in E} (e_v - e_w) (e_v - e_w)^t.
    $$
    On the other hand,
    \begin{equation}
        \label{full_unity}
        I_{\mathrm{im}(L)} = (L^+)^{1/2} L (L^+)^{1/2} = \sum_{(v,w) \in E} q_{vw} q_{vw}^t,
    \end{equation}
    where $q_{vw} = (L^+)^{1/2} (e_v - e_w)$. Since $q_{vw} \in \mathrm{im}(L)$, we can
    consider (\ref{full_unity}) only in $\mathrm{im}(L)$: it gives us a decomposition of
    $(n-1)\times(n-1)$ identity patrix into the sum of $m$ 1-rank matrices.
    This decomposition is very convenient for both of the algorithms we are about to
    present.

    What does it mean for a weighted graph $G'$ to approximate the Laplacian of $G$ well?
    It means that
    \begin{equation}
        \label{approx_laplacian}
        \sup_x \left|\frac{x^t \tilde{L} x - x^t L x}{x^t L x}\right|\leq \eps,
    \end{equation}
    where $L$ is the Laplacian of $G$, and $\tilde{L}$ is the Laplacian of $G'$
    (technically, we defined the Laplacian only for undirected graphs, but for weighted
    ones the definition remains essentially the same).
    Clearly, it sufficient to check~(\ref{approx_laplacian}) only for
    $x \in \mathrm{im}(L)$.
    If we denote $y = L^{1/2} x$, then we have to check whether
    $$
        \sup_{y \in \mathrm{im}(L)} \left|\frac{y^t (L^+)^{1/2}
            \tilde{L} (L^+)^{1/2} y - y^t y}{y^t y}\right| \leq \eps.
    $$
    But this condition is equivalent to
    \begin{equation}
        \label{closeness}
        \|(L^+)^{1/2} \tilde{L} (L^+)^{1/2} - I_{\mathrm{im}(L)}\|_2 \leq \eps.
    \end{equation}
    If we require $G'$ to be a (weighted) subgraph of $G$, then
    \begin{equation}
        \label{subgraph}
        \tilde{L} = \sum_{(v,w)\in E} m_{vw} (e_v - e_w) (e_v - e_w)^t,
    \end{equation}
    where $m_{vw}$ is a weight of an edge $(v, w)$.

    So, combining~(\ref{full_unity}), (\ref{closeness}) and (\ref{subgraph}),
    we reduce the problem of spectral sparsification to the following clean
    linear-algebraic problem.

    \begin{problem}[Sparsification of Partition of Unity]
        \label{spu}
        Suppose that
        $$
            I = \sum_{i=1}^m q_i q_i^t,
        $$
        where $q_i \in \mathbb{R}^n$
        and $\eps > 0$~--- some parameter.
        What is the minimum $N$ such that there exists a subset $S \subseteq [m]$ with
        $|S| = N$ and non-negative numbers $\alpha_i \geq 0$ for every $i \in [m]$ such that
        $$
            \|I - \sum_{i \in S} \alpha_i q_i q_i^t\|_2 \leq \eps?
        $$
    \end{problem}

    \subsection{Sparsification by Sampling}

    The paper~\cite{SS11} solves Problem~\ref{spu} using random sampling.
    Suppose that we pick $i_1, i_2, \ldots, i_N \in [m]$ independently so that
    $\mathbf{Pr}[i_j = i] = p_i$, where $p_i$ are some probabilities.
    We want to choose $p_i$ and $N$ so that
    $\|I - \sum_{j \in [N]} q_{i_j} q_{i_j}^t / (N\cdot p_{i_j})\|_2 \leq \eps$
    with high (or at least with non-zero) probability (we need the normalizing
    factor $1 / Np_{i_j}$ in order for the random sum to be equal to $I$ in expectation).

    What $N$ and $p_i$ should we choose, if we want $N$ to be relatively small?
    In~\cite{R96,RV07} the following celebrated result was proved.
    \begin{theorem}
        \label{rv_inequality}
        Let $y$ be an $n$-dimensional vector-valued random variable.
        Suppose that $\|y\|_2 \leq M$ with probability $1$ and
        $\|\mathbf{E}[yy^t]\|_2 \leq 1$. Then,
        \begin{equation}
            \label{rv_formula}
            \mathbf{E}[\|\frac{1}{N} \sum_{i = 1}^{N} y_i y_i^t - \mathbf{E}[y_i y_i^t]\|_2]
            \leq O\left(M \sqrt{\frac{\log N}{N}}\right),
        \end{equation}
        where $y_i$ -- independent copies of $y$.
    \end{theorem}

    We apply this theorem to the following random variable: we choose $i \in [m]$
    with probability $p_i$ and then set $y := q_i / \sqrt{p_i}$.
    In order to make the RHS of~(\ref{rv_formula}) as small as possible, we should
    minimize $M = \max_i \|q_i\|_2 / \sqrt{p_i}$. Clearly, in order to do this
    we should choose $p_i$ proportional to $\|q_i\|_2^2$.
    In this case
    $$
        M = \sqrt{\sum_{i=1}^m \|q_i\|_2^2} =
        \sqrt{\sum_{i=1}^m \mathrm{tr}(q_i q_i^t)} =
        \sqrt{\mathrm{tr}\left(\sum_{i=1}^m q_i q_i^t\right)} = \sqrt{n}.
    $$
    So, from Theorem~\ref{rv_inequality} we see that we can take $N = O(n \log n / \eps^2)$
    and get the deviation of the spectral norm at most by $\eps / 2$ in expectation.
    Using Markov inequality we see that with probability at least $1/2$ the deviation
    is at most $\eps$. Repeating the sampling process several times independently
    we can boost the probability of success.

    \subsection{Near-linear Time Laplacian Sparsification}

    The argument from the previous section gives a near-linear time algorithm for solving
    Problem~\ref{spu}. But our initial motivation was to sparsify the Laplacian of a graph
    $G$. The catch here is that our reduction to Problem~\ref{spu} requires a computation
    of $(L^+)^{1/2}$, which we do not know how to do faster than in time $O(n^{2.373})$.
    So, in order to be able to use the argument from the previous section, we have to solve
    the following problem: for every edge $(v, w) \in E$ compute
    $r_{vw} := \|q_{vw}\|_2^2 =
    \|(L^+)^{1/2}(e_v - e_w)\|_2^2 = (e_v - e_w)^t L^+ (e_v - e_w)$.
    Despite we do not know how to compute $r_{vw}$'s exactly in near-linear time we can
    observe, that actually any constant-factor approximations for $r_{vw}$'s are sufficient
    (it will only affect the hidden constant factor in the resulting number of edges).
    Thus, we face the following problem:
    \begin{problem}
        Compute the constant-factor approximations to
        $$
            r_{vw} = (e_v - e_w)^t L^+ (e_v - e_w).
        $$
    \end{problem}

    Let us use the following presentation of $L$: $L = M^t M$, where $M$ is the incidence
    matrix of $G$ ($m$ by $n$ matrix whose rows correspond to the edges, a row that
    corresponds to $(v, w)$ equals to $(e_v - e_w)^t$). Thus,
    $$
        r_{vw} = (e_v - e_w)^t L^+ (e_v - e_w)^t = (e_v - e_w)^t L^+ L L^+ (e_v - e_w) =
        \|M L^+ (e_v - e_w)\|_2^2.
    $$
    So, we are interested in pairwise $\ell_2$-distances between the columns of
    $M L^+$.
    To compute $M L^+$ we can, for example multiply each row of $M$ by $L^+$. It turns
    out that there is a way to do it in near-linear time per row using solvers for graph
    Laplacians (for example, the one described in~\cite{KMP10}).
    But the problem is that we have $m$ rows, so the overall running time would be
    $\tilde{O}(m^2)$.
    Luckily, we can fix this problem. Since we care only about approximate pairwise
    distances we can employ the dimension reduction technique~\cite{DG03}:
    if $Q$ is a random
    $O(\log n) \times m$ matrix whose entries are i.i.d. Gaussians, then with
    high probability the pairwise distances between the columns of $QML^{+}$ approximate
    those from $ML^+$. We can multiply $Q$ by $M$ in time $O(m \log n)$, and then
    invoke the Laplacian solver $O(\log n)$ times.

    \subsection{Iterative Sparsification}

    Unlike~\cite{SS11}, a completely different approach for solving Problem~\ref{spu}
    is used in~\cite{BSS09}.
    If in the RHS of~\ref{rv_formula} there were not $\sqrt{\log N}$ factor, the exactly
    the same sampling procedure would give sparsifiers with $O(n / \eps^2)$ terms.
    Unfortunately, this factor is unavoidable.
    If $y$ is a random basis vector, then, clearly, we need at least $\Omega(n \log n)$
    samples to choose each vector at least once.
    Thus, no matter what probabilities do we use for (independent) sampling, we can not
    overcome the bound $O(n \log n)$.

    So, to get the optimal $O(n / \eps^2)$ one has to use ``deterministic'' methods.
    We will use the iterative approach: on each step we keep track to a current approximation
    $A$ of the identity, and update $A$ by adding a multiple $\alpha q_i q_i^t$ for some
    carefully chosen $\alpha$ and $i$ (this update procedure is called rank-$1$ update).
    Since we want that our resulting matrix to have all eigenvalues within
    $[1 - \eps; 1 + \eps]$, let us first study how eigenvalues behave under rank-$1$
    updates.

    Since the trace of a matrix equals to the sum of its eigenvalues we see that if
    we increase $A$ by $vv^t$, then the sum of eigenvalues increases by $\|v\|_2^2$.
    So, ``on average'' every eigenvalue increases by $\|v\|_2^2 / n$.
    If we managed to get many of such ``uniform'' increases, then we would have been done:
    after some time all eigenvalues will be within $(1 \pm \eps)$ of each other, so after
    the proper normalization they will be within $[1 - \eps; 1 + \eps]$.

    Unfortunately, there is no guarantee that all eigenvalues will be increased by
    (approximately) the same amount.
    Here comes the brilliant idea of Batson, Spielman, and Srivastava~\cite{BSS09}.
    They consider the following ``barrier'' potential functions which allow us to conclude
    that all eigenvalues grow on a steady pace.

    \begin{definition}
        Let use denote by $\lambda_1, \lambda_2, \ldots, \lambda_n$ the eigenvalues of
        a square matrix $A$. Then,
        \begin{eqnarray*}
            L_{x}(A) &:=& \sum_{i} \frac{1}{\lambda_i - x}; \\
            U_{x}(A) &:=& \sum_{i} \frac{1}{x - \lambda_i}.
        \end{eqnarray*}
        Let us call $L_x(A)$ \emph{lower barrier} and $U_x(A)$~--- upper barrier,
        respectively.
    \end{definition}

    Clearly, $L_x(A)$ is increasing in $x$ and $U_x(A)$ is, on the other hand, decreasing.
    These barriers blow up, whenever $x$ equals to some eigenvalue.

    The overall strategy for sparsification will be the following:
    let $\eps_L > 0$, $\eps_U > 0$, $\delta_L > 0$, $\delta_U > 0$, $l_0$, $u_0$ be some
    constants. We have the successive approximations $A_0, A_1, \ldots, A_N$ such that
    \begin{itemize}
        \item $A_0 = 0$;
        \item $A_i = A_{i-1} + \alpha q_j q_j^t$ for some $\alpha \geq 0$ and $j \in [m]$;
        \item $L_{l_0 + i \cdot \delta_L}(A_i) \leq L_{l_0 +
            (i - 1) \cdot \delta_L}(A_{i-1})
              \leq \eps_L$;
        \item $R_{r_0 + i \cdot \delta_R}(A_i) \leq R_{r_0 +
            (i - 1) \cdot \delta_R}(A_{i-1})
              \leq \eps_R$;
        \item all eigenvalues of $A_i$ are within $(l_0 + i \cdot \delta_L; r_0 + i \cdot
              \delta_R)$.
    \end{itemize}

    In the end of the day we will choose the parameters as follows: $\delta_L = 1$,
    $\delta_R = 1 + \Theta(\eps)$, $l_0 = -\Theta(n / \eps)$, $r_0 = \Theta(n / \eps)$.
    Clearly, it implies that after $O(n / \eps^2)$ steps all the eigenvalues will be
    within $(1 \pm \eps)$ of each other, so we are done.

    The main question is, of course, how to choose $\alpha$ and $q_j$ to maintain all the
    invariants. The plan of the argument is the following: first, we derive
    (rather ugly looking) conditions on $j$ that imply that there exists an $\alpha$ such
    that if we set $A_i := A_{i-1} + \alpha q_j q_j^t$, then all the invariants will be
    maintained. Then, we prove that if we choose $j$ uniformly at random, then
    ``on expectation'' these conditions will hold.
    On the algorithmic side it implies that we can try all possible $j$'s and choose the
    one that works.

    So, suppose we want to set $A_i := A_{i-1} + \alpha q_j q_j^t$ for some $\alpha \geq 0$
    and $j \in [m]$. The next Lemma quantifies what $\alpha$ we should choose so that
    $L_{l_0 + i \cdot \delta_L}(A_i) \leq L_{l_0 + (i - 1) \cdot \delta_L}(A_{i-1})$
    and $R_{r_0 + i \cdot \delta_R}(A_i) \leq R_{r_0 + (i - 1) \cdot \delta_R}(A_{i-1})$.

    For compactness let us denote $l_i := l_0 + i \cdot \delta_L$,
    $r_i := r_0 + i \cdot \delta_R$.

    \begin{lemma}
      $L_{l_i}(A_i) \leq L_{l_{i-1}}(A_{i-1})$
      iff
      $$
        0 < \frac{1}{\alpha} \leq -q_j^t (A_{i-1} - l_i I)^{-1} q_j +
        \frac{q_j^t (A_{i-1} - l_i I)^{-2}q_j}{L_{l_i}(A_{i-1}) - L_{l_{i-1}}(A_{i-1})}
        =: \sigma(j).
      $$
      $R_{r_i}(A_i) \leq R_{r_{i-1}}(A_{i-1})$ iff
      $$
        \frac{1}{\alpha} \geq q_j^t (r_i I - A_{i-1})^{-1} q_j +
        \frac{q_j^t (r_i I - A_{i-1})^{-2} q_j}{R_{r_{i-1}}(A_{i-1}) - R_{r_i}(A_{i-1})}
        =: \tau(j). 
      $$
    \end{lemma}
    \begin{proof}
        \footnote{TODO!!!!!}
    \end{proof}

    Argue about eigenvalues!!!!
    
    Now it is left to prove that we can choose the parameters such that on every 
    iteration there exists
    $j \in [m]$ such that $\sigma(j) \geq \tau(j)$.
    We do this using the averaging argument. We show that one choose the parameters such
    that $\sum_j \sigma(j) \geq \sum_j \tau(j)$.

    We have
    \begin{eqnarray*}
        \sum_j \sigma(j) &=& -\sum_t \frac{1}{\lambda_t - l_i} +
        \frac{\sum_t \frac{1}{(\lambda_t - l_i)^2}}
        {\sum_t \frac{1}{\lambda_t - l_i} - \sum_t \frac{1}{\lambda_t - l_{i-1}}};\\ 
        \sum_j \tau(j) &=& \sum_t \frac{1}{r_i - \lambda_t} +
        \frac{\sum_t \frac{1}{(r_i - \lambda_t)^2}}
        {\sum_t \frac{1}{r_{i-1} - \lambda_t} - \sum_t \frac{1}{r_i - \lambda_t}},\\ 
    \end{eqnarray*}
    where $\lambda_t$ are the eigenvalues of $A_{i-1}$.

    One can prove that $\sum_j \sigma(j) \geq 1 / \delta_L - \eps_L$,
    $\sum_j \tau(j) \leq 1 / \delta_R + \eps_R$.

    \section{Vertex Sparsification}
    \bibliographystyle{alpha}
    \bibliography{ir}
\end{document}
