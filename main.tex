\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{ccfonts}
\usepackage[T1]{fontenc}

\renewcommand{\bfdefault}{sbc}

\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\eps}{\varepsilon}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\XComment}[1]{}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}

\begin{document}
    \title{Spectral Sparsification of Undirected Graphs}
    \author{Ilya Razenshteyn\\\texttt{ilyaraz@mit.edu}}
    \maketitle

    \section{Introduction}

    One recurring topic in Graph Theory is how to approximate a given graph with much sparser one while preserving certain
    properties. A very useful and clean
    notion of \emph{cut sparsification} was introduced in~\cite{BK96} by Bencz\'{u}r and Karger.
    They proved the following result.

    \begin{theorem}[\cite{BK96}]
        \label{benczur_karger}
        Let $\eps > 0$ be some sufficiently small parameter.
        For every undirected weighted graph $G$ there exists another undirected weighted graph $\tilde{G}$ with the
        same set of vertices and only $O(n \log n / \eps^2)$ edges such that every cut value in $\tilde{G}$
        is within $(1 \pm \eps)$ of the corresponding value in $G$. Moreover, $\tilde{G}$ can be found in time
        $O(m \log^3 n)$ with high probability.
    \end{theorem}

    Two obvious applications of Theorem~\ref{benczur_karger} are the following:
    \begin{itemize}
        \item by sparsifying graph and then running the algorithm of Goldberg and Rao~\cite{GR98} we can find
        $(1 + \eps)$-approximations to the maximum $(s, t)$-flow and the minimum $(s, t)$-cut in time
        $O(m \log^3 n + n^{3/2} \log n \log(nU) / \eps^3)$, where $U$ is an upper bound on the capacities in our network;
        \item by sparsifying graph and then running the algorithm of Klein, Stein and Tardos~\cite{KST90} we can find
        an $O(\log n)$-approximation to the sparsest cut in time $O((m + n^2) \log^3 n)$.
    \end{itemize}

    The proof of Theorem~\ref{benczur_karger} goes along the following lines. The first naive idea is to employ uniform
    random sampling (in case of unweighted graphs).
    It turns out that one can prove that it works, provided that the minimum cut is large enough.
    To overcome this problem with small cuts one has to sample edges with probabilities inversely proportional to their
    \emph{strong connectivities}.

    In~\cite{ST11} Spielman and Teng considered a singificantly stronger notion of sparsification: the so called
    \emph{spectral sparsification}. To define it we need a notion of \emph{graph Laplacian}.

    \begin{definition}
        Let $G = (V, E, w)$ be an undirected weighted graph. Then the following quadratic form is called \emph{Laplacian}
        of $G$:
        $$
            \mathcal{L}_G(x) = \sum_{(u, v) \in E} w(u, v) (x_u - x_v)^2.
        $$
    \end{definition}
    \begin{definition}
        We say that $\tilde{G}$ is a spectral sparsifier of $G$, if for every $x \in \Rbb^n$
        $$
            \mathcal{L}_{\tilde{G}}(x) \in (1 \pm \eps) \mathcal{L}_G(x).
        $$
    \end{definition}

    To see why the notion of spectral sparsification is stronger than that of cut sparsification it is sufficient to observe
    that $\mathcal{L}_G(\mathbf{1}_S)$ is exactly equal to the cut value of $(S, \bar{S})$.
    But we require much more: to preserve $\mathcal{L}_G(x)$ for every $x$, not only for the indicators.

    We will survey two constructions of spectral sparsifiers. The first is from~\cite{SS11}, which gives sparsifiers
    with $O(n \log n / \eps^2)$ edges
    matching the bound of Theorem~\ref{benczur_karger}. Moreover, these sparsifiers can be found
    in near-linear time. The second construction is from~\cite{BSS09}: it gives sparsifiers with $O(n / \eps^2)$ edges,
    but the algorithm for building them is much slower (nevertheless, even the existance of near-linear-sized spectral
    sparsifiers is higly non-trivial).

    Let us mention several applications of spectral sparsification.

    First, since spectral sparsifiers approximately preserve spectra of Laplacians, one can look at sparsifiers of the
    complete graph $K_n$ as expander graphs~\cite{HLW06}. Technically speaking, this analogy is quite vague, because
    expanders are constant-degree regular graphs, on the other hand, even the best construction of spectral sparsifiers
    from~\cite{BSS09}, despite giving a constant \emph{average} degree, does not provide graphs with constant maximum
    degree. Nevetheless, it is proved in~\cite{BSS09} that spectral sparsifiers of $K_n$ share several properties with
    constant-degree expanders.

    Second, spectral sparsification can be used to prove the existence of good \emph{approximate John decompositions}
    (see~\cite{B97} for an excellent introduction to Convex Geometry and~\cite{N11} for a great exposition of geometric
    applications of~\cite{BSS09}).

    Third, spectral sparsification can be used for dimension reduction in $\ell_1$-spaces.
    The classic theorem of Johnson--Lindenstrauss~\cite{DG03}
    says that if we have $n$ vectors in $\mathbb{R}^d$, then we can map them into $\mathbb{R}^{O(\log n / \eps^2)}$
    so that to $(1\pm\eps)$-preserve their pairwise $\ell_2$-distances.
    This theorem has immense number of applications (in particular, we will use it to find spectral sparsifiers quickly).
    It would be great to extend it to $\ell_1$, but in the series of papers~\cite{BC05}, \cite{LN04}, \cite{ACNN11},
    \cite{R11a} it was proved that one needs dimension to be essentially linear in $n$ in this case.
    Nevertheless, in~\cite{T90}
    it was proved that one can reduce dimension to $O(n \log n / \eps^2)$. Using spectral sparsification one can sharpen
    this bound to $O(n / \eps^2)$ (see~\cite{N11} for the exposition).

    \subsection{Organization of the report}

    Our report is organized as follows. First, in Section~\ref{linear_algebra} we review the definitions and the results
    we will need further. Then, in Section~\ref{reduction_unity} we show how to reduce the problem of Laplacian
    sparsification to the following clean linear-algebraic problem.
    \begin{problem}[Sparsification of a decomposition of identity]
        Let $q_1, q_2, \ldots, q_m$ be the set of vectors from $\mathbb{R}^n$ such that
        \begin{equation}
            \label{identity_decomposition}
            \sum_{i=1}^m q_i q_i^t = I.
        \end{equation}
        Let $\eps > 0$ be some parameter. We want to sparsify~(\ref{identity_decomposition}): find $N \ll m$ and
        the numbers $\alpha_1, \alpha_2, \ldots, \alpha_m \geq 0$ such that there are at most $N$ non-zero
        $\alpha$'s and all the eigenvalues of $\sum_i \alpha_i q_i q_i^t$ are within $[1 - \eps; 1 + \eps]$.

        What is the smallest $N = N(n, \eps)$ can we guarantee?
    \end{problem}

    In Section~\ref{resistances} we show the solution for this problem from~\cite{SS11} that is based on sampling
    (it turns out that one should sample $q_i$'s with probabilities proportional to $\|q_i\|_2^2$)
    and gives $N = O(n \log n / \eps^2)$, in Section~\ref{iterative} we show the ``deterministic'' approach of
    $\cite{BSS09}$ that gives $N = O(n / \eps^2)$.

    For clarity we assume that we want to sparsify an unweighted connected graph.

    \section{Preliminaries}
    \label{linear_algebra}

    Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix. 
    \begin{theorem}[Eigendecomposition]
        All the eigenvalues
        of $A$ are real and there exists an orthonormal basis of $\mathbb{R}^n$
        that consists of $A$'s eigenvectors.

        We can write
        $$
            A = \sum_{i=1}^n \lambda_i u_i u_i^t,
        $$
        where $\lambda_1, \lambda_2, \ldots, \lambda_n$ are the eigenvalues of $A$, and
        $u_1, u_2, \ldots, u_n$ are the corresponding eigenvectors that form an orthonormal
        basis of $\mathbb{R}^n$. This decomposition is called \emph{eigendecomposition}
        of $A$.
    \end{theorem}

    We can use the notion of eigendecomposition to introduce a pseudoinverse of $A$.

    \begin{definition}[Pseudoinverse]
        A \emph{pseudoinverse} of $A$ is the following matrix:
        $$
            A^+ := \sum_{i : \lambda_i \ne 0} \frac{1}{\lambda_i} u_i u_i^t.
        $$
    \end{definition}

    Clearly, if $A$ is non-degenerate (equivalently, $\lambda_i \ne 0$ for every $i$),
    then $A^+ = A^{-1}$.
    On the other hand, for every symmetric $A$
    $$
        A A^+ = A^+ A = I_{\mathrm{im}(A)},
    $$
    where $I_{\mathrm{im}(A)}$ is a linear operator that maps vectors from $\mathrm{ker}(A)$
    to zero and maps vectors from $\mathrm{im}(A)$ to themselves (this definition is correct,
    because we can consider a decomposition
    $\mathbb{R}^n = \mathrm{ker}(A) \oplus \mathrm{im}(A)$).

    If a symmetric matrix is positive semi-definite (equivalently, all the eigenvalues
    are non-negative), then we can consider its square root.

    \begin{definition}[Square Root of a PSD Matrix]
        $$
            A^{1/2} := \sum_{i} \sqrt{\lambda_i} u_i u_i^t.
        $$
    \end{definition}
    Clearly, $A^{1/2} A^{1/2} = A$.

    During the presentation of spectral sparsification algorithms we will use the following
    matrix norm crucially.

    \begin{definition}[Spectral Norm]
        $$
            \|A\|_2 := \max_{x} \frac{\|Ax\|_2}{\|x\|_2}.
        $$
    \end{definition}

    For symmetric matrices spectral norm is just the maximum absolute value of an eigenvalue.
    If $I$ is the identity matrix, then $\|A - I\|_2 \leq \eps$ iff all the eigenvalues of
    $A$ lie within $[1 - \eps; 1 + \eps]$.

    We will need the following ``law of large numbers'' for matrix-valued random variables proved by Rudelson and Vershynin
    in~\cite{R96}, \cite{RV07}.
    \begin{theorem}
        \label{rv_inequality}
        Let $y$ be an $n$-dimensional vector-valued random variable.
        Suppose that $\|y\|_2 \leq M$ with probability $1$ and
        $\|\mathbf{E}[yy^t]\|_2 \leq 1$. Then,
        \begin{equation}
            \label{rv_formula}
            \mathbf{E}\left[\left\|\frac{1}{N} \sum_{i = 1}^{N} y_i y_i^t - \mathbf{E}[y_i y_i^t]\right\|_2\right]
            \leq O\left(M \sqrt{\frac{\log N}{N}}\right),
        \end{equation}
        where $y_i$ -- independent copies of $y$.
    \end{theorem}

    We need the following peculiar identity for the so-called rank-$1$ updates.

    \begin{theorem}[Sherman-Morrison formula]
        \label{sherman_morrison}
        If $A$ is square and invertible and $v^tA^{-1}u \ne -1$, then
        $$
            (A + uv^t)^{-1} = A^{-1} - \frac{A^{-1}uv^tA^{-1}}{1 + v^tA^{-1}u}.
        $$
    \end{theorem}

    \section{Reduction to Sparsification of a Decomposition of Identity}
    \label{reduction_unity}

    If $G$ is an undirected unweighted graph, then, clearly
    $$
        \mathcal{L}_G(x) = x^t L x,
    $$
    where $L_{vv}$ equals to the degree of $v$, $L_{vw} = -1$, if $(v, w) \in E$, and
    $L_{vw} = 0$, otherwise.

    Clearly, $\mathbf{1} \in \mathrm{ker}(L)$. Moreover, it is easy to prove that
    $\mathrm{ker}(L) = \langle \mathbf{1} \rangle$ iff $G$ is connected.
    In this case $\mathrm{im}(L) = \langle\mathbf{1}\rangle^{\perp}$.

    We have the following natural rank-$1$ decomposition of $L$:
    \begin{equation}
        \label{ldec}
        L = \sum_{(v, w) \in E} (e_v - e_w) (e_v - e_w)^t.
    \end{equation}

    Now let us understand what does it mean for a graph $\tilde{G}$ to approximate
    $G$ in the sense of Laplacian. Let us denote $\tilde{L}$ the Laplacian of $G$.
    We want that for every $x \in \Rbb^n$
    \begin{equation}
        \label{app}
        x^t \tilde{L} x \in (1 \pm \eps) x^t L x.
    \end{equation}
    Clearly, if $x \in \mathrm{ker}(L)$, then there is nothing to worry about, since
    both sides of~(\ref{app}) are equal to zero. So, it is only left to ensure
    that~(\ref{app}) holds for $x \in \mathrm{im}(L)$.
    In this case we can write $x = (L^+)^{1/2} y$ for
    $y = L^{1/2} x \in \mathrm{im}(L)$.
    Thus, we want that for every $y \in \mathrm{im}(L)$
    $$
        y^t (L^+)^{1/2} \tilde{L} (L^+)^{1/2} y \in (1 \pm \eps) y^t y.
    $$
    It is equivalent to
    $$
        \|(L^+)^{1/2} \tilde{L} (L^+)^{1/2} - I_{\mathrm{im}(L)}\|_2 \leq \eps.
    $$
    But this is exactly an instance of Problem~\ref{identity_decomposition} for
    the following decomposition:
    $$
        I_{\mathrm{im}(L)} = (L^+)^{1/2} L (L^+)^{1/2} = \sum_{(v,w) \in E} q_{vw} q_{vw}^t,
    $$
    where $q_{vw} = (L^+)^{1/2} (e_v - e_w)$ (we pass from $\mathbb{R}^n$ to
    $\mathrm{im}(L) = \langle \mathbf{1} \rangle^{\perp}$ here, we also use~(\ref{ldec})).

    \section{Sparsification by Sampling}
    \label{resistances}

    \subsection{Existential Argument}

    In this section we present the argument from~\cite{SS11}, which gives a solution
    for Problem~\ref{identity_decomposition} with $N = O(n \log n / \eps^2)$.
    Suppose we want to sparsify a decomposition of indentity with independent sampling.
    What probabilities for the terms $q_i q_i^t$ should we choose?
    Let us denote them by $p_1, p_2, \ldots, p_m$.
    Let us assume that we make $N$ independent samples $i_1, i_2, \ldots, i_m$
    from $[m]$ with probabilities $p_i$ and form a sum
    $$
        X = \frac{1}{N} \sum_{j=1}^N \frac{q_{i_j} q_{i_j}^t}{p_{i_j}}.
    $$
    Clearly, $\mathbf{E}[X] = I$. Now let us use Theorem~\ref{rv_inequality} to understand
    what should we take $N$ equal to. We have
    $$
        M = \max_i \frac{\|q_i\|_2}{\sqrt{p_i}}.
    $$
    In order to make $M$ as small as possible we should take $p_i$ proportional to
    $\|q_i\|_2^2$. Thus, we have
    $$
        M = \sqrt{\sum_{i=1}^m \|q_i\|_2^2} =
        \sqrt{\sum_{i=1}^m \mathrm{tr}(q_i q_i^t)} =
        \sqrt{\mathrm{tr}\left(\sum_{i=1}^m q_i q_i^t\right)} = \sqrt{n}.
    $$
    Now we just invoke Theorem~\ref{rv_inequality} and get $N = O(n \log n / \eps^2)$
    immediately.

    \textbf{Remark:} suprisingly enough, it turns out that for the graph Laplacian case
    $\|q_{vw}\|_2^2$ is equal to the effetive resistance of an edge $(v, w)$.
    This intuition is heavily used in several recent developments. Futhermore, there is a strong similarity between this
    sparsification procedure and that of from~\cite{BK96}: effective resistances reflect edge connectivities.

    \subsection{Near-linear Time Algorithm (a High-Level Idea)}

    Despite it is proved in the previous section that there exists a sparsifier of size
    $O(n \log n / \eps^2)$, we still do not have a fast algorithm for finding one.
    Indeed, in order to sample edges we must know $\|q_{vw}\|_2^2$ for every edge
    $(v, w) \in E$. Let us recall that
    $$
        \|q_{vw}\|_2^2 = \|(L^+)^{1/2} (e_v - e_w)\|_2^2 =
        (e_v - e_w)^t L^+ (e_v - e_w).
    $$
    Unfortunately, we do not know how to compute $L^+$ quickly, but luckily it is sufficient
    for us to compute $\|q_{vw}\|_2^2$'s approximately (for instance, any
    $O(1)$-approximation is sufficient).
    Let us observe that $L = M^t M$, where $M$ is the incidence matrix of the graph
    ($m \times n$ matrix, where rows correspond to edges, a row that corresponds to
    $(v, w)$ is equal to $e_v - e_w$). Now let us rewrite
    $$
        \|q_{vw}\|_2^2 = (e_v - e_w)^t L^+ (e_v - e_w) =
        \|ML^+(e_v - e_w)\|_2^2,
    $$
    so we are interested in pairwise $\ell_2$-distances between the columns of $ML^+$.
    One could compute the rows of $ML^+$ by multiplying $m$ rows of $M$ by $L^+$.
    Amazingly, there exists a way to do it in near-linear time approximately
    (see~\cite{KMP10} for more details). Unfortunately, this is still not sufficient
    for our purposes, because there are $m$ rows and we get $\tilde{O}(m^2)$ time in total.

    Here comes into the play the celebrated result of Johnson and Lindenstrauss
    (see~\cite{DG03} for a good exposition): if we take $Q$ to be a (properly normalized)
    $O(\log n) \times n$ random matrix with i.i.d. Gaussian entries, then
    the pairwise distances between the columns of $ML^+$ are approximately equal to that of
    $QML^+$. Fortunately, we can multiply $Q$ by $M$ in time $\tilde{O}(m)$, and then
    invoke the solver from~\cite{KMP10} only $O(\log n)$ times.

    \section{Iterative Sparsification}
    \label{iterative}

    To improve the bound $O(n \log n / \eps^2)$ one should switch from (independent) edge sampling to something else.
    The reason is that in Theorem~\ref{rv_inequality} the factor $\sqrt{\log N}$ is essential: consider the case,
    when $y$ is a random basis vector. Then, clearly, we need at least $\Omega(n \log n)$ samples to choose each
    vector at least once. Thus, no matter what probabilities are we using for independent edge sampling, we can not
    overcome the bound $O(n \log n)$.
    So, to get the optimal $O(n / \eps^2)$ bound one has to use ``deterministic'' methods.

    We will use the iterative approach: on each step we keep track of a current matrix $A$
    and perform a rank-$1$ update of it --- set $A := A + \alpha q_j q_j^t$ for some carefully chosen $\alpha \geq 0$ and
    $j \in [m]$. Our goal is that the eigenvalues of $A$ are within $(1 \pm \eps)$ of each other.

    Since $\mathrm{tr}(A + vv^t) = \mathrm{tr}(A) + \|v\|_2^2$, the eigenvalues of $A$ increase by
    $\|v\|_2^2 / n$ ``on average''. If all the eigenvalues always increase by the same amount, we are done: after some time
    all of them will be close to each other.
    Unfortunately, we are not guaranteed to have ``uniform'' increases.

    Here comes the brilliant idea of Batson, Spielman and Srivastava~\cite{BSS09}.
    They consider the following ``barrier'' potential functions, which will allow us to control the pace of eigenvalues'
    growth.

    \begin{definition}
        Let $A$ be a square matrix. Then
        \begin{eqnarray*}
            L_x(A) &=& \mathrm{tr}(A - xI)^{-1} = \sum_i \frac{1}{\lambda_i(A) - x}\\
            U_x(A) &=& \mathrm{tr}(xI - A)^{-1} = \sum_i \frac{1}{x - \lambda_i(A)}\\
        \end{eqnarray*}
    \end{definition}

    The idea of~\cite{BSS09} is to maintain $l$ and $u$ such that $L_l(A)$ and $U_u(A)$ are small, and all the eigenvalues are
    within $(l; u)$, which will allow us
    to conclude that we can perform a rank-$1$ update to $A$ and increase $l$ and $u$ by certain amounts.

    More formally, let $\delta_L, \delta_U, \eps_L, \eps_U, l_0, u_0$ be some parameters, moreover,
    $\delta_L, \delta_U, \eps_L, \eps_U$ are positive.
    We build a sequence of matrices $A_0, A_1, \ldots, A_N$ such that
    \begin{itemize}
        \item $A_0 = 0$;
        \item $A_i = A_{i-1} + \alpha q_j q_j^t$ for some $\alpha > 0$ and $j \in [m]$;
        \item $L_{l_i}(A_i) \leq L_{l_{i-1}}(A_{i-1}) \leq \eps_L$, where $l_i = l_0 + i \cdot \delta_L$;
        \item $U_{u_i}(A_i) \leq U_{u_{i-1}}(A_{i-1}) \leq \eps_U$, where $u_i = u_0 + i \cdot \delta_U$;
        \item all the eigenvalues of $A_i$ lie within $(l_i; u_i)$.
    \end{itemize}
    We will show that for these invariants to hold it is sufficient for the following relations to hold.
    \begin{itemize}
        \item $\eps_L = -n / l_0$, $\eps_U = n / u_0$;
        \item $1 / \delta_U + \eps_U \leq 1 / \delta_L - \eps_L$;
        \item $\eps_L \leq 1 / \delta_L$.
    \end{itemize}
    One can see that we can take $\eps_L = \Theta(\eps)$, $\eps_U = \Theta(\eps)$, $\delta_L = 1$,
    $\delta_U = 1 + \Theta(\eps)$, $l_0 = -\Theta(n / \eps)$, $u_0 = \Theta(n / \eps)$ and get a desired sparsifier
    after $O(n / \eps^2)$ steps.

    Suppose we want to set $A_i = A_{i-1} + \alpha vv^t$. As a simple consequence of Theorem~\ref{sherman_morrison} we
    get the following formulae.
    \begin{eqnarray*}
        L_{l_i}(A_i) &=& L_{l_i}(A_{i-1}) - \frac{v^t (A_{i-1} - l_iI)^{-2} v}{1 / \alpha + v^t (A_{i-1} - l_i I)^{-1} v}\\
        U_{u_i}(A_i) &=& U_{u_i}(A_{i-1}) + \frac{v^t (u_iI - A_{i-1})^{-2} v}{1 / \alpha - v^t (u_iI - A_{i-1})^{-1} v}\\
    \end{eqnarray*}

    Using these formulae it is straightforward to prove the following Lemmas.

    \begin{lemma}
        \label{lower_shift}
        $L_{l_i}(A_i) \leq L_{l_{i-1}}(A_{i-1})$ iff
        $$
            0 < \frac{1}{\alpha} \leq \sigma(v) := -v^t(A_{i-1}-l_iI)^{-1}v +
            \frac{v^t(A_{i-1}-l_iI)^{-2}v}{L_{l_i}(A_{i-1}) - L_{l_{i-1}}(A_{i-1})}.
        $$
    \end{lemma}

    \begin{lemma}
        \label{upper_shift}
        $U_{u_i}(A_i) \leq U_{u_{i-1}}(A_{i-1})$ iff
        $$
            \frac{1}{\alpha} \geq \tau(v) := v^t(u_iI - A_{i-1})^{-1}v +
            \frac{v^t(u_iI - A_{i-1})^{-2}v}{U_{u_{i-1}}(A_{i-1}) - U_{u_i}(A_{i-1})}.
        $$
    \end{lemma}

    As a result we prove the following Theorem.
    \begin{theorem}
        If $\tau(v) \leq \sigma(v)$, then there exists an $\alpha > 0$ such that if we set $A_i := A_{i-1} + \alpha vv^t$,
        then
        \begin{itemize}
            \item $L_{l_i}(A_i) \leq L_{l_{i-1}}(A_{i-1})$;
            \item $U_{u_i}(A_i) \leq U_{u_{i-1}}(A_{i-1})$;
            \item all the eigenvalues of $A_i$ are within $(l_i; u_i)$.
        \end{itemize}
    \end{theorem}
    \begin{proof}
        The first and the second statement are trivial corollaries of Lemma~\ref{lower_shift} and~\ref{upper_shift}.
        Thus, it is left to argue about eigenvalues.
        Clearly, all the eigenvalues of $A_{i-1}$ (and, thus, of $A_i$) are at least
        $l_{i-1} + 1 / \eps_L \leq l_{i-1} + \delta_L = l_i$, because $L_{l_{i-1}}(A_{i-1}) \leq \eps_L$.
        Since $L_{l_i}(A_i)$ is finite, we have that all eigenvalues of $A_i$ are strictly greater than $l_i$.
        Now let us argue about the upper bound. From Lemma~\ref{upper_shift} it follows that if $\alpha$ is sufficiently
        small, then $U_{u_i}(A_i)$ is finite (and even small). If the largest eigenvalue of $A_i$ is at least $u_i$,
        it means that for some small $\alpha$ $U_{u_i}(A_i)$ blows up, thus we have a contradiction.
    \end{proof}

    Now it is left to prove that there exists $j \in [m]$ such that $\tau(q_j) \leq \sigma(q_j)$.
    For this we prove the following theorem.

    \begin{theorem}
        If $1 / \delta_U + \eps_U \leq 1 / \delta_L - \eps_L$, then
        $$
            \sum_j \tau(q_j) \leq \sum_j \sigma(q_j)
        $$
    \end{theorem}
    \begin{proof}
        One can prove using that
        \begin{eqnarray*}
            \sum_j \tau(q_j) &=& \sum_{t}(u_i - \lambda_t)^{-1} + \frac{\sum_t(u_i - \lambda_t)^{-2}}
            {\sum_t (u_{i-1} - \lambda_t)^{-1} - \sum_t (u_i - \lambda_t)^{-1}}\\
            \sum_j \sigma(q_j) &=& -\sum_t(\lambda_t - l_i)^{-1} + \frac{\sum_t(\lambda_t - l_i)^{-2}}
            {\sum_t(\lambda_t - l_i)^{-1} - \sum_t(\lambda_t - l_{i-1})^{-1}}.\\
        \end{eqnarray*}
        One can fairly easy show that $\sum_j \tau(q_j) \leq \eps_U + 1 / \delta_U$. With some work
        one can show that $\sum_j \sigma(q_j) \geq 1 / \delta_L - \eps_L$. The desired inequality follows.
    \end{proof}

    \section{Acknowledgements}

    I am very thankful to Richard Peng for enlightening discussions.
    \bibliographystyle{alpha}
    \bibliography{ir}
\end{document}
