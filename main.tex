\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{ccfonts}
\usepackage[T1]{fontenc}

\renewcommand{\bfdefault}{sbc}

\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\eps}{\varepsilon}
\newcommand{\set}[1]{\left\{#1\right\}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}

\begin{document}
    \title{Edge and Vertex Sparsification of Undirected Graphs}
    \date{}
    \maketitle

    \section{Introduction}
    We survey two sparsification procedures for undirected graphs that respect
    (to some extent) values of flows and cuts.
    The first is spectral sparsification,
    which was initially introduced by Spielman and Teng~\cite{ST11} and was
    inspired by a combinatorial notion of cut sparsification by Bencz\'{u}r and
    Karger~\cite{BK96}.
    The second is vertex sparsification discovered by Leighton and
    Moitra~\cite{M09}~\cite{LM10}.
    \subsection{Spectral Sparsification}
    \subsubsection{Cut Sparsifiers}
    Let us first define cut sparsifiers.
    \begin{definition}
        Let $G = (V, E, w)$ be an undirected weighted graph.
        We say that $G' = (V, E', w')$ is an $\eps$-cut sparsifier
        for $G$ if all cut values of $G'$ are within
        $1 \pm \eps$ the corresponding cut values of $G$.
    \end{definition}
    The goal is to construct a sparsifier for a fixed $\eps$ with as few edges as possible.
    The following result is due to Bencz\'{u}r and Karger~\cite{BK96}.
    \begin{theorem}
        \label{benczur_karger}
        Every graph has $\eps$-cut sparsifier with $O(n \log n / \eps^2)$ edges.
        Moreover, such sparsifier can be found in time
        $O(m \log^3 n + n \log n / \eps^2)$ with high probability.
    \end{theorem}
    One of the immediate applications of this powerful theorem is the following.
    Using the algorithm of Goldberg and Rao~\cite{GR98} we can find a maximum $(s, t)$-flow
    (and a minimum $(s, t)$-cut)
    in time $\tilde{O}(m^{3/2})$, if all capacities are polynomially bounded.
    If we are fine with $(1 \pm \eps)$-approximation, then we can first sparsify the network,
    and then run the algorithm. The overall running time is
    $\tilde{O}(m + n^{3/2} / \eps^3)$.

    Theorem~\ref{benczur_karger} is proved using randomized sampling. The naive idea would
    be to use uniform subsampling. The problem with it is that it can fail miserably:
    if $G$ has a bridge, then we have to save it, but in order to ensure that this will be
    the case, we have to set the probability of subsampling very close to $1$.
    So, we have to take \emph{connectivities} of edges into account.
    It turns out that if we define the notion of connectivity appropriately and sample
    edges with probabilities inversely proportional to connectivities, then we can get
    Theorem~\ref{benczur_karger}.

    The notion of cut sparsification can be reformulated (and then significantly extended)
    using linear-algebraic tools.
    \subsubsection{Graph Laplacian}
    For any undirected graph $G = (V, E, w)$ we can consider the following quadratic
    form:
    $$
        L_G(x) = \sum_{\set{i, j} \in E} w_{ij} (x_i - x_j)^2.
    $$
    Clearly, if $x$ is an indicator for some subset $S \subseteq V$, then $L(x)$
    is exactly equal to the size of the cut $(S, \bar{S})$.
    Thus, we can reformulate Theorem~\ref{benczur_karger} as follows: for every weighted
    undirected graph $G$ there exists a graph $G'$ with $O(n \log n / \eps^2)$ edges such
    that for every $x \in \set{0, 1}^n$ we have $L_{G'}(x) \in (1 \pm \eps) L_G(x)$.
    The quadratic form (and the corresponding matrix) $L_G$ is called Laplacian of $G$.

    It turns out that we can preserve not only $L_G(x)$ for $x \in \set{0, 1}^n$, but
    we can preserve $L_G(x)$ for any $x$! Let us give the corresponding definition:
    \begin{definition}
        Let $G = (V, E, w)$ be an undirected weighted graph.
        We say that $G' = (V, E', w')$ is an $\eps$-spectral sparsifier
        for $G$ if for every $x \in \Rbb^n$ we have $L_{G'}(x) \in (1 \pm \eps) L_G(x)$.
    \end{definition}

    The notion of spectral sparsification was first introduced by Spielman and Teng
    in~\cite{ST11}. They proved that for every graph there exists an $\eps$-spectral
    sparsifier with $\tilde{O}(m \cdot \mathrm{poly}(1 / \eps))$ edges.
    Moreover, it can be found in time $\tilde{O}(m \cdot \mathrm{poly}(1 / \eps))$
    with high probability.
    The proof roughly goes as follows: first, we prove that there exists a partition of
    a graph such that every part is well-connected, and there are at most half of the
    edges between the parts. Then, we prove that if a graph is well-connected then
    one can sample edges with probabilities inversely proportional to the sum of degrees
    of endpoints. Then, we recurse and sparsify the edges between the parts.
    The proof if technically very involved.
    Moreover, the exponent of poly-logarithmic factor is large for this
    construction.

    In the follow-up work Spielman and Srivastava~\cite{SS11} singificantly simplifed
    and strenghtened the result from~\cite{ST11}.
    They proved the following theorem.
    \begin{theorem}
        For every graph there exists an $\eps$-sparsifier with $O(n \log n / \eps^2)$
        edges. Moreover, it can be constructed in time $\tilde{O}(m)$.
    \end{theorem}
    The proof is very simple: we sample edges with probabilities proportional to
    \emph{the effective resistances}. Say we replace every edge with a 1 Ohm resistor.
    Then for every edge we can define an effective resistance between its endpoints.
    It sounds surprising that the effective resistances are of use for spectral
    sparsification, but at least the more effective resistance is the less connected
    endpoints are, so we should try to preserve edges with high effective resistances.
    It turns out that effective resistances and Laplacian are closely connected, and,
    in some sense, the effective resistances are the right probabilities for sampling,
    if one wants to preserve Laplacian.
    \subsection{Vertex Sparsification}
    \section{Spectral Sparsification}
    \textbf{The following text will be rewritten}

    We want that for every $x$ we have $(1 - \eps) x^t L_G x \leq x^t L_{G'} x \leq (1 + \eps) x^t L_G x$.
    If $x \in \mathrm{ker}(L_G)$, then $x^t L_G x = x^t L_{G'} x = 0$ (here we use that $G$ is connected).
    So, we want
    $$
        \sup_{x \in \mathrm{im}(L_G)} \left|\frac{x^t (L_{G'} - L_G) x}{x^t L_G x} \right| \leq \eps.
    $$
    Since $\mathrm{im}(L_G) = \mathrm{im}(L_G^+)$, we want
    $$
        \sup_{y} \left|\frac{y^t (L_G^+)^{1/2} (L_{G'} - L_G) (L_G^+)^{1/2} y}{y^t y}\right| \leq \eps.
    $$
    Or, equivalently,
    $$
        \|(L_G^+)^{1/2} L_{G'} (L_G^+)^{1/2} - I_{\mathrm{im}(L_G)}\|_2 \leq \eps.
    $$
    \textbf{Remark:} actually, $I_{\mathrm{im}(L_G)} = I_n - J_n / n$.

    \begin{theorem}[Rudelson--Vershynin]
        If $u$ is a vector random variable distributed over $\mathbb{R}^n$ such that
        \begin{itemize}
            \item $\mathbf{Pr}[\|u\|_2 \leq M] = 1$;
            \item $\|\mathbf{E}[uu^t]\|_2 \leq 1$,
        \end{itemize}
        then
        $$
            \mathbf{E}\left[\left\|\frac{1}{N} \sum_{i=1}^N u_i u_i^t -
            \mathbf{E}[uu^t]\right\|_2\right] \leq O\left(M \sqrt{\frac{\log N}{N}}\right),
        $$
        where $u_i$'s are independent copies of $u$.
    \end{theorem}

    Suppose we want to form a graph $G'$ by sampling edges of $G$ with probabilities
    $p_{vw}$.
    We want to apply Rudelson--Vershynin, but to make it as efficient as possible, we want
    to have small enough $M$.
    Clearly, if we set $L_G' = \sum_{i=1}^N y_i y_i^t / N$, where $y_i$ is equal to
    $(e_v - e_w) / \sqrt{p_{vw}}$ with probability $p_{vw}$, then we have
    $\mathbf{E}[L_G'] = L_G$.

    So, we set $u = (L_G^+)^{1/2} (e_v - e_w) / \sqrt{p_{vw}}$ with probability $p_{vw}$.
    Clearly, $\|E[uu^t]\|_2 = \|I_{\mathrm{im}(L_G)}\|_2 = 1$. Also,
    $\|u\|_2 \leq \max_{vw} \sqrt{R_{vw} / p_{vw}} =: M$.
    If we want to minimize $M$ we should take $p$'s proportional to $R$'s.
    Since
    $$
        \sum_{vw} R_{vw} = \mathrm{tr}(ML^+M^t) = \mathrm{tr}(M^tML^+) =
        \mathrm{tr}(LL^+) = \mathrm{tr}(I_{\mathrm{im}(L)}) = n - 1,
    $$
    we have $M = \sqrt{n - 1}$. So, from Rudelson--Vershynin we get that we need
    $O(n \log n / \eps^2)$ samples.

    \subsection{Algorithms}

    From the existential argument we get an algorithm with running time $O(n^{\omega})$.
    The bottleneck is computing $L^+$.
    But what we actually need is effective resistances:
    $$
        R_{vw} = (e_v - e_w)^t L^+ (e_v - e_w) = \|ML^+ (e_v - e_w)\|_2^2.
    $$
    So, we want to be able to estimate $\ell_2$-distance between the columns of
    $ML^+$. We can use the dimension reduction paradigm: let us multiply $ML^+$ by
    a random $\pm 1$-matrix $A$: so we get $O(\log n) \times n$ matrix $AML^+$ that
    $O(1)$-preserves pairwise $\ell_2$ distances between columns.
    We can compute $AM$ in $\tilde{O}(m)$ time, and our problem reduces to $\tilde{O}(1)$
    computations of the form $L^+ x$ for some vectors $x$. For this we can employ
    Laplacian solvers.

    For any Laplacian $L$ and any $x$ we can find a vector $y$ such that
    $$
        \|y - L^+x\|_L \leq \eps \|L^+x\|_L,
    $$
    where $\|u\|_L := \sqrt{u^t L u}$, in time $\tilde{O}(m \log(1 / \eps))$. It turns out
    that for us it is sufficient to take $\eps = n^{-\Theta(1)}$, so the total running time
    is $\tilde{O}(m)$.

    \subsection{Laplacian Solver}

    \subsection{Sparsifiers with linear number of edges}
    \cite{BSS09}

    \section{Spectral Sparsification}

    \subsection{Preliminaries}

    \subsubsection{Linear Algebra 101}

    Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix. 
    \begin{theorem}[Eigendecomposition]
        All the eigenvalues
        of $A$ are real and there exists an orthonormal basis of $\mathbb{R}^n$
        that consists of $A$'s eigenvectors.

        We can write
        $$
            A = \sum_{i=1}^n \lambda_i u_i u_i^t,
        $$
        where $\lambda_1, \lambda_2, \ldots, \lambda_n$ are the eigenvalues of $A$, and
        $u_1, u_2, \ldots, u_n$ are the corresponding eigenvectors that form an orthonormal
        basis of $\mathbb{R}^n$. This decomposition is called \emph{eigendecomposition}
        of $A$.
    \end{theorem}

    We can use the notion of eigendecomposition to introduce a pseudoinverse of $A$.

    \begin{definition}[Pseudoinverse]
        A \emph{pseudoinverse} of $A$ is the following matrix:
        $$
            A^+ := \sum_{i : \lambda_i \ne 0} \frac{1}{\lambda_i} u_i u_i^t.
        $$
    \end{definition}

    Clearly, if $A$ is non-degenerate (equivalently, $\lambda_i \ne 0$ for every $i$),
    then $A^+ = A^{-1}$.
    On the other hand, for every symmetric $A$
    $$
        A A^+ = A^+ A = I_{\mathrm{im}(A)},
    $$
    where $I_{\mathrm{im}(A)}$ is a linear operator that maps vectors from $\mathrm{ker}(A)$
    to zero and maps vectors from $\mathrm{im}(A)$ to themselves (this definition is correct,
    because we can consider a decomposition
    $\mathbb{R}^n = \mathrm{ker}(A) \oplus \mathrm{im}(A)$).

    If a symmetric matrix is positive semi-definite (equivalently, all the eigenvalues
    are non-negative), then we can consider its square root.

    \begin{definition}[Square Root of a PSD Matrix]
        $$
            A^{1/2} := \sum_{i} \sqrt{\lambda_i} u_i u_i^t.
        $$
    \end{definition}
    Clearly, $A^{1/2} A^{1/2} = A$.

    During the presentation of spectral sparsification algorithms we will use the following
    matrix norm crucially.

    \begin{definition}[Spectral Norm]
        $$
            \|A\|_2 := \max_{x} \frac{\|Ax\|_2}{\|x\|_2}.
        $$
    \end{definition}

    For symmetric matrices spectral norm is just the maximum absolute value of an eigenvalue.
    If $I$ is the identity matrix, then $\|A - I\|_2 \leq \eps$ iff all the eigenvalues of
    $A$ lie within $[1 - \eps; 1 + \eps]$.

    \subsubsection{Graph Laplacian}

    Let $G = (V, E)$ be an undirected graph.

    \begin{definition}[Laplacian]
        The following $n \times n$ matrix $L$ is called the Laplacian of $G$:
        $$
            x^t L x = \sum_{(v,w) \in E} (x_v - x_w)^2. 
        $$
    \end{definition}

    Clearly, $L_{vv}$ equals to the degree of $v$ and $L_{vw} = -1$, if $(v, w) \in E$,
    and $0$, otherwise.
    Since $L$ is positive-semidefinite, all $L$'s eigenvalues are non-negative. Let
    $0 \leq \lambda_1 \leq \lambda_2 \leq \ldots \leq \lambda_n$ be the eigenvalues of $L$.

    \begin{lemma}
        For every graph $G$ $\lambda_1 = 0$. Moreover, $G$ is connected iff $\lambda_2 > 0$.
    \end{lemma}
    \begin{proof}
        Clearly, $\mathbf{1}^t L \mathbf{1} = 0$, so, $\lambda_1 = 0$ and the corresponding
        eigenvector is $\mathbf{1} / \sqrt{n}$.

        If there is a non-zero
        vector within the kernel of $L$ that is orthogonal to $\mathbf{1}$
        it is easy to see that there are no edges connecting negative and positive
        coordinates, so $G$ is disconnected. Conversely, if $G$ is connected and
        $x^t L x = 0$, then, clearly, $x = \alpha \mathbf{1}$.
    \end{proof}

    So, if $G$ is connected, then $\mathrm{ker}(L) = \langle \mathbf{1} \rangle$
    and $\mathrm{im}(L) = \langle \mathbf{1} \rangle^{\perp}$.

    It turns out that $\lambda_2$ determines how well-connected $G$ is. The larger
    $\lambda_2$ is, the better connected $G$ is, and vice versa. This fact is known
    as Cheeger's inequality.

    We can decompose $L$ as follows:
    $$
        L = \sum_{(v,w) \in E} (e_v - e_w) (e_v - e_w)^t.
    $$
    On the other hand,
    \begin{equation}
        \label{full_unity}
        I_{\mathrm{im}(L)} = (L^+)^{1/2} L (L^+)^{1/2} = \sum_{(v,w) \in E} q_{vw} q_{vw}^t,
    \end{equation}
    where $q_{vw} = (L^+)^{1/2} (e_v - e_w)$. Since $q_{vw} \in \mathrm{im}(L)$, we can
    consider (\ref{full_unity}) only in $\mathrm{im}(L)$: it gives us a decomposition of
    $(n-1)\times(n-1)$ identity patrix into the sum of $m$ 1-rank matrices.
    This decomposition is very convenient for both of the algorithms we are about to
    present.

    What does it mean for a weighted graph $G'$ to approximate the Laplacian of $G$ well?
    It means that
    \begin{equation}
        \label{approx_laplacian}
        \sup_x \left|\frac{x^t \tilde{L} x - x^t L x}{x^t L x}\right|\leq \eps,
    \end{equation}
    where $L$ is the Laplacian of $G$, and $\tilde{L}$ is the Laplacian of $G'$
    (technically, we defined the Laplacian only for undirected graphs, but for weighted
    ones the definition remains essentially the same).
    Clearly, it sufficient to check~(\ref{approx_laplacian}) only for
    $x \in \mathrm{im}(L)$.
    If we denote $y = L^{1/2} x$, then we have to check whether
    $$
        \sup_{y \in \mathrm{im}(L)} \left|\frac{y^t (L^+)^{1/2}
            \tilde{L} (L^+)^{1/2} y - y^t y}{y^t y}\right| \leq \eps.
    $$
    But this condition is equivalent to
    \begin{equation}
        \label{closeness}
        \|(L^+)^{1/2} \tilde{L} (L^+)^{1/2} - I_{\mathrm{im}(L)}\|_2 \leq \eps.
    \end{equation}
    If we require $G'$ to be a (weighted) subgraph of $G$, then
    \begin{equation}
        \label{subgraph}
        \tilde{L} = \sum_{(v,w)\in E} m_{vw} (e_v - e_w) (e_v - e_w)^t,
    \end{equation}
    where $m_{vw}$ is a weight of an edge $(v, w)$.

    So, combining~(\ref{full_unity}), (\ref{closeness}) and (\ref{subgraph}),
    we reduce the problem of spectral sparsification to the following clean
    linear-algebraic problem.

    \begin{problem}[Sparsification of Partition of Unity]
        \label{spu}
        Suppose that
        $$
            I = \sum_{i=1}^m q_i q_i^t,
        $$
        where $q_i \in \mathbb{R}^n$
        and $\eps > 0$~--- some parameter.
        What is the minimum $N$ such that there exists a subset $S \subseteq [m]$ with
        $|S| = N$ and non-negative numbers $\alpha_i \geq 0$ for every $i \in [m]$ such that
        $$
            \|I - \sum_{i \in S} \alpha_i q_i q_i^t\|_2 \leq \eps?
        $$
    \end{problem}

    \subsection{Sparsification by Sampling}

    The paper~\cite{SS11} solves Problem~\ref{spu} using random sampling.
    Suppose that we pick $i_1, i_2, \ldots, i_N \in [m]$ independently so that
    $\mathbf{Pr}[i_j = i] = p_i$, where $p_i$ are some probabilities.
    We want to choose $p_i$ and $N$ so that
    $\|I - \sum_{j \in [N]} q_{i_j} q_{i_j}^t / (N\cdot p_{i_j})\|_2 \leq \eps$
    with high (or at least with non-zero) probability (we need the normalizing
    factor $1 / Np_{i_j}$ in order for the random sum to be equal to $I$ in expectation).

    What $N$ and $p_i$ should we choose, if we want $N$ to be relatively small?
    In~\cite{R96,RV07} the following celebrated result was proved.
    \begin{theorem}
        \label{rv_inequality}
        Let $y$ be an $n$-dimensional vector-valued random variable.
        Suppose that $\|y\|_2 \leq M$ with probability $1$ and
        $\|\mathbf{E}[yy^t]\|_2 \leq 1$. Then,
        \begin{equation}
            \label{rv_formula}
            \mathbf{E}[\|\frac{1}{N} \sum_{i = 1}^{N} y_i y_i^t - \mathbf{E}[y_i y_i^t]\|_2]
            \leq O\left(M \sqrt{\frac{\log N}{N}}\right),
        \end{equation}
        where $y_i$ -- independent copies of $y$.
    \end{theorem}

    We apply this theorem to the following random variable: we choose $i \in [m]$
    with probability $p_i$ and then set $y := q_i / \sqrt{p_i}$.
    In order to make the RHS of~(\ref{rv_formula}) as small as possible, we should
    minimize $M = \max_i \|q_i\|_2 / \sqrt{p_i}$. Clearly, in order to do this
    we should choose $p_i$ proportional to $\|q_i\|_2^2$.
    In this case
    $$
        M = \sqrt{\sum_{i=1}^m \|q_i\|_2^2} =
        \sqrt{\sum_{i=1}^m \mathrm{tr}(q_i q_i^t)} =
        \sqrt{\mathrm{tr}\left(\sum_{i=1}^m q_i q_i^t\right)} = \sqrt{n}.
    $$
    So, from Theorem~\ref{rv_inequality} we see that we can take $N = O(n \log n / \eps^2)$
    and get the deviation of the spectral norm at most by $\eps / 2$ in expectation.
    Using Markov inequality we see that with probability at least $1/2$ the deviation
    is at most $\eps$. Repeating the sampling process several times independently
    we can boost the probability of success.

    \subsection{Near-linear Time Laplacian Sparsification}

    The argument from the previous section gives a near-linear time algorithm for solving
    Problem~\ref{spu}. But our initial motivation was to sparsify the Laplacian of a graph
    $G$. The catch here is that our reduction to Problem~\ref{spu} requires a computation
    of $(L^+)^{1/2}$, which we do not know how to do faster than in time $O(n^{2.373})$.
    So, in order to be able to use the argument from the previous section, we have to solve
    the following problem: for every edge $(v, w) \in E$ compute
    $r_{vw} := \|q_{vw}\|_2^2 =
    \|(L^+)^{1/2}(e_v - e_w)\|_2^2 = (e_v - e_w)^t L^+ (e_v - e_w)$.
    Despite we do not know how to compute $r_{vw}$'s exactly in near-linear time we can
    observe, that actually any constant-factor approximations for $r_{vw}$'s are sufficient
    (it will only affect the hidden constant factor in the resulting number of edges).
    Thus, we face the following problem:
    \begin{problem}
        Compute the constant-factor approximations to
        $$
            r_{vw} = (e_v - e_w)^t L^+ (e_v - e_w).
        $$
    \end{problem}

    Let us use the following presentation of $L$: $L = M^t M$, where $M$ is the incidence
    matrix of $G$ ($m$ by $n$ matrix whose rows correspond to the edges, a row that
    corresponds to $(v, w)$ equals to $(e_v - e_w)^t$). Thus,
    $$
        r_{vw} = (e_v - e_w)^t L^+ (e_v - e_w)^t = (e_v - e_w)^t L^+ L L^+ (e_v - e_w) =
        \|M L^+ (e_v - e_w)\|_2^2.
    $$
    So, we are interested in pairwise $\ell_2$-distances between the columns of
    $M L^+$.
    To compute $M L^+$ we can, for example multiply each row of $M$ by $L^+$. It turns
    out that there is a way to do it in near-linear time per row using solvers for graph
    Laplacians (for example, the one described in~\cite{KMP10}).
    But the problem is that we have $m$ rows, so the overall running time would be
    $\tilde{O}(m^2)$.
    Luckily, we can fix this problem. Since we care only about approximate pairwise
    distances we can employ the dimension reduction technique~\cite{DG03}:
    if $Q$ is a random
    $O(\log n) \times m$ matrix whose entries are i.i.d. Gaussians, then with
    high probability the pairwise distances between the columns of $QML^{+}$ approximate
    those from $ML^+$. We can multiply $Q$ by $M$ in time $O(m \log n)$, and then
    invoke the Laplacian solver $O(\log n)$ times.

    \subsection{Iterative Sparsification}

    \section{Vertex Sparsification}
    \bibliographystyle{alpha}
    \bibliography{ir}
\end{document}
