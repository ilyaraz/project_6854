\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{ccfonts}
\usepackage[T1]{fontenc}

\renewcommand{\bfdefault}{sbc}

\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\eps}{\varepsilon}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\XComment}[1]{}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}

\begin{document}
    \title{Spectral Sparsification of Undirected Graphs}
    \author{Ilya Razenshteyn\\\texttt{ilyaraz@mit.edu}}
    \maketitle

    \section{Introduction}

    One recurring topic in Graph Theory is how to approximate a given graph with much sparser one while preserving certain
    properties. A very useful and clean
    notion of \emph{cut sparsification} was introduced in~\cite{BK96} by Bencz\'{u}r and Karger.
    They proved the following result.

    \begin{theorem}[\cite{BK96}]
        \label{benczur_karger}
        Let $\eps > 0$ be some sufficiently small parameter.
        For every undirected weighted graph $G$ there exists another undirected weighted graph $\tilde{G}$ with the
        same set of vertices and only $O(n \log n / \eps^2)$ edges such that every cut value in $\tilde{G}$
        is within $(1 \pm \eps)$ of the corresponding value in $G$. Moreover, $\tilde{G}$ can be found in time
        $O(m \log^3 n)$ with high probability.
    \end{theorem}

    Two obvious applications of Theorem~\ref{benczur_karger} are the following:
    \begin{itemize}
        \item by sparsifying graph and then running the algorithm of Goldberg and Rao~\cite{GR98} we can find
        $(1 + \eps)$-approximations to the maximum $(s, t)$-flow and the minimum $(s, t)$-cut in time
        $O(m \log^3 n + n^{3/2} \log n \log(nU) / \eps^3)$, where $U$ is an upper bound on the capacities in our network;
        \item by sparsifying graph and then running the algorithm of Klein, Stein and Tardos~\cite{KST90} we can find
        an $O(\log n)$-approximation to the sparsest cut in time $O((m + n^2) \log^3 n)$.
    \end{itemize}

    The proof of Theorem~\ref{benczur_karger} goes along the following lines. The first naive idea is to employ uniform
    random sampling (in case of unweighted graphs).
    It turns out that one can prove that it works, provided that the minimum cut is large enough.
    To overcome this problem with small cuts one has to sample edges with probabilities proportional to their
    \emph{strong connectivities}.

    In~\cite{ST11} Spielman and Teng considered a singificantly stronger notion of sparsification: the so called
    \emph{spectral sparsification}. To define it we need a notion of \emph{graph Laplacian}.

    \begin{definition}
        Let $G = (V, E, w)$ be an undirected weighted graph. Then the following quadratic form is called \emph{Laplacian}
        of $G$:
        $$
            \mathcal{L}_G(x) = \sum_{(u, v) \in E} w(u, v) (x_u - x_v)^2.
        $$
    \end{definition}
    \begin{definition}
        We say that $\tilde{G}$ is a spectral sparsifier of $G$, if for every $x \in \Rbb^n$
        $$
            \mathcal{L}_{\tilde{G}}(x) \in (1 \pm \eps) \mathcal{L}_G(x).
        $$
    \end{definition}

    To see why the notion of spectral sparsification is stronger than that of cut sparsification it is sufficient to observe
    that $\mathcal{L}_G(\mathbf{1}_S)$ is exactly equal to the cut value of $(S, \bar{S})$.
    But we require much more: to preserve $\mathcal{L}_G(x)$ for every $x$, not only for the indicators.

    We will survey two constructions of spectral sparsifiers. The first is from~\cite{SS11}, which gives sparsifiers
    with $O(n \log n / \eps^2)$ edges
    matching the bound of Theorem~\ref{benczur_karger}. Moreover, these sparsifiers can be found
    in near-linear time. The second construction is from~\cite{BSS09}: it gives sparsifiers with $O(n / \eps^2)$ edges,
    but the algorithm for building them is much slower (nevertheless, even the existance of near-linear-sized spectral
    sparsifiers is higly non-trivial).

    Let us mention several applications of spectral sparsification.

    First, since spectral sparsifiers approximately preserve spectra of Laplacians, one can look at sparsifiers of the
    complete graph $K_n$ as expander graphs~\cite{HLW06}. Technically speaking, this analogy is quite vague, because
    expanders are constant-degree regular graphs, on the other hand, even the best construction of spectral sparsifiers
    from~\cite{BSS09}, despite giving a constant \emph{average} degree, does not provide graphs with constant maximum
    degree. Nevetheless, it is proved in~\cite{BSS09} that spectral sparsifiers of $K_n$ share several properties with
    constant-degree expanders.

    Second, spectral sparsification can be used to prove the existence of good \emph{approximate John decompositions}
    (see~\cite{B97} for an excellent introduction to Convex Geometry and~\cite{N11} for a great exposition of geometric
    applications of~\cite{BSS09}).

    Third, spectral sparsification can be used for dimension reduction in $\ell_1$-spaces.
    The classic theorem of Johnson--Lindenstrauss~\cite{DG03}
    says that if we have $n$ vectors in $\mathbb{R}^d$, then we can map them into $\mathbb{R}^{O(\log n / \eps^2)}$
    so that to $(1\pm\eps)$-preserve their pairwise $\ell_2$-distances.
    This theorem has immense number of applications (in particular, we will use it to find spectral sparsifiers quickly).
    It would be great to extend it to $\ell_1$, but in the series of papers~\cite{BC05}, \cite{LN04}, \cite{ACNN11},
    \cite{R11a} it was proved that one needs dimension to be essentially linear in $n$ in this case.
    Nevertheless, in~\cite{T90}
    it was proved that one can reduce dimension to $O(n \log n / \eps^2)$. Using spectral sparsification one can sharpen
    this bound to $O(n / \eps^2)$ (see~\cite{N11} for the exposition).

    \XComment{
    \section{Introduction}
    We survey two sparsification procedures for undirected graphs that respect
    (to some extent) values of flows and cuts.
    The first is spectral sparsification,
    which was initially introduced by Spielman and Teng~\cite{ST11} and was
    inspired by a combinatorial notion of cut sparsification by Bencz\'{u}r and
    Karger~\cite{BK96}.
    The second is vertex sparsification discovered by Leighton and
    Moitra~\cite{M09}~\cite{LM10}.
    \subsection{Spectral Sparsification}
    \subsubsection{Cut Sparsifiers}
    Let us first define cut sparsifiers.
    \begin{definition}
        Let $G = (V, E, w)$ be an undirected weighted graph.
        We say that $G' = (V, E', w')$ is an $\eps$-cut sparsifier
        for $G$ if all cut values of $G'$ are within
        $1 \pm \eps$ the corresponding cut values of $G$.
    \end{definition}
    The goal is to construct a sparsifier for a fixed $\eps$ with as few edges as possible.
    The following result is due to Bencz\'{u}r and Karger~\cite{BK96}.
    \begin{theorem}
        \label{benczur_karger}
        Every graph has $\eps$-cut sparsifier with $O(n \log n / \eps^2)$ edges.
        Moreover, such sparsifier can be found in time
        $O(m \log^3 n + n \log n / \eps^2)$ with high probability.
    \end{theorem}
    One of the immediate applications of this powerful theorem is the following.
    Using the algorithm of Goldberg and Rao~\cite{GR98} we can find a maximum $(s, t)$-flow
    (and a minimum $(s, t)$-cut)
    in time $\tilde{O}(m^{3/2})$, if all capacities are polynomially bounded.
    If we are fine with $(1 \pm \eps)$-approximation, then we can first sparsify the network,
    and then run the algorithm. The overall running time is
    $\tilde{O}(m + n^{3/2} / \eps^3)$.

    Theorem~\ref{benczur_karger} is proved using randomized sampling. The naive idea would
    be to use uniform subsampling. The problem with it is that it can fail miserably:
    if $G$ has a bridge, then we have to save it, but in order to ensure that this will be
    the case, we have to set the probability of subsampling very close to $1$.
    So, we have to take \emph{connectivities} of edges into account.
    It turns out that if we define the notion of connectivity appropriately and sample
    edges with probabilities inversely proportional to connectivities, then we can get
    Theorem~\ref{benczur_karger}.

    The notion of cut sparsification can be reformulated (and then significantly extended)
    using linear-algebraic tools.
    \subsubsection{Graph Laplacian}
    For any undirected graph $G = (V, E, w)$ we can consider the following quadratic
    form:
    $$
        L_G(x) = \sum_{\set{i, j} \in E} w_{ij} (x_i - x_j)^2.
    $$
    Clearly, if $x$ is an indicator for some subset $S \subseteq V$, then $L(x)$
    is exactly equal to the size of the cut $(S, \bar{S})$.
    Thus, we can reformulate Theorem~\ref{benczur_karger} as follows: for every weighted
    undirected graph $G$ there exists a graph $G'$ with $O(n \log n / \eps^2)$ edges such
    that for every $x \in \set{0, 1}^n$ we have $L_{G'}(x) \in (1 \pm \eps) L_G(x)$.
    The quadratic form (and the corresponding matrix) $L_G$ is called Laplacian of $G$.

    It turns out that we can preserve not only $L_G(x)$ for $x \in \set{0, 1}^n$, but
    we can preserve $L_G(x)$ for any $x$! Let us give the corresponding definition:
    \begin{definition}
        Let $G = (V, E, w)$ be an undirected weighted graph.
        We say that $G' = (V, E', w')$ is an $\eps$-spectral sparsifier
        for $G$ if for every $x \in \Rbb^n$ we have $L_{G'}(x) \in (1 \pm \eps) L_G(x)$.
    \end{definition}

    The notion of spectral sparsification was first introduced by Spielman and Teng
    in~\cite{ST11}. They proved that for every graph there exists an $\eps$-spectral
    sparsifier with $\tilde{O}(m \cdot \mathrm{poly}(1 / \eps))$ edges.
    Moreover, it can be found in time $\tilde{O}(m \cdot \mathrm{poly}(1 / \eps))$
    with high probability.
    The proof roughly goes as follows: first, we prove that there exists a partition of
    a graph such that every part is well-connected, and there are at most half of the
    edges between the parts. Then, we prove that if a graph is well-connected then
    one can sample edges with probabilities inversely proportional to the sum of degrees
    of endpoints. Then, we recurse and sparsify the edges between the parts.
    The proof if technically very involved.
    Moreover, the exponent of poly-logarithmic factor is large for this
    construction.

    In the follow-up work Spielman and Srivastava~\cite{SS11} singificantly simplifed
    and strenghtened the result from~\cite{ST11}.
    They proved the following theorem.
    \begin{theorem}
        For every graph there exists an $\eps$-sparsifier with $O(n \log n / \eps^2)$
        edges. Moreover, it can be constructed in time $\tilde{O}(m)$.
    \end{theorem}
    The proof is very simple: we sample edges with probabilities proportional to
    \emph{the effective resistances}. Say we replace every edge with a 1 Ohm resistor.
    Then for every edge we can define an effective resistance between its endpoints.
    It sounds surprising that the effective resistances are of use for spectral
    sparsification, but at least the more effective resistance is the less connected
    endpoints are, so we should try to preserve edges with high effective resistances.
    It turns out that effective resistances and Laplacian are closely connected, and,
    in some sense, the effective resistances are the right probabilities for sampling,
    if one wants to preserve Laplacian.
    \subsection{Vertex Sparsification}
    \section{Spectral Sparsification}
    \textbf{The following text will be rewritten}

    We want that for every $x$ we have $(1 - \eps) x^t L_G x \leq x^t L_{G'} x \leq (1 + \eps) x^t L_G x$.
    If $x \in \mathrm{ker}(L_G)$, then $x^t L_G x = x^t L_{G'} x = 0$ (here we use that $G$ is connected).
    So, we want
    $$
        \sup_{x \in \mathrm{im}(L_G)} \left|\frac{x^t (L_{G'} - L_G) x}{x^t L_G x} \right| \leq \eps.
    $$
    Since $\mathrm{im}(L_G) = \mathrm{im}(L_G^+)$, we want
    $$
        \sup_{y} \left|\frac{y^t (L_G^+)^{1/2} (L_{G'} - L_G) (L_G^+)^{1/2} y}{y^t y}\right| \leq \eps.
    $$
    Or, equivalently,
    $$
        \|(L_G^+)^{1/2} L_{G'} (L_G^+)^{1/2} - I_{\mathrm{im}(L_G)}\|_2 \leq \eps.
    $$
    \textbf{Remark:} actually, $I_{\mathrm{im}(L_G)} = I_n - J_n / n$.

    \begin{theorem}[Rudelson--Vershynin]
        If $u$ is a vector random variable distributed over $\mathbb{R}^n$ such that
        \begin{itemize}
            \item $\mathbf{Pr}[\|u\|_2 \leq M] = 1$;
            \item $\|\mathbf{E}[uu^t]\|_2 \leq 1$,
        \end{itemize}
        then
        $$
            \mathbf{E}\left[\left\|\frac{1}{N} \sum_{i=1}^N u_i u_i^t -
            \mathbf{E}[uu^t]\right\|_2\right] \leq O\left(M \sqrt{\frac{\log N}{N}}\right),
        $$
        where $u_i$'s are independent copies of $u$.
    \end{theorem}

    Suppose we want to form a graph $G'$ by sampling edges of $G$ with probabilities
    $p_{vw}$.
    We want to apply Rudelson--Vershynin, but to make it as efficient as possible, we want
    to have small enough $M$.
    Clearly, if we set $L_G' = \sum_{i=1}^N y_i y_i^t / N$, where $y_i$ is equal to
    $(e_v - e_w) / \sqrt{p_{vw}}$ with probability $p_{vw}$, then we have
    $\mathbf{E}[L_G'] = L_G$.

    So, we set $u = (L_G^+)^{1/2} (e_v - e_w) / \sqrt{p_{vw}}$ with probability $p_{vw}$.
    Clearly, $\|E[uu^t]\|_2 = \|I_{\mathrm{im}(L_G)}\|_2 = 1$. Also,
    $\|u\|_2 \leq \max_{vw} \sqrt{R_{vw} / p_{vw}} =: M$.
    If we want to minimize $M$ we should take $p$'s proportional to $R$'s.
    Since
    $$
        \sum_{vw} R_{vw} = \mathrm{tr}(ML^+M^t) = \mathrm{tr}(M^tML^+) =
        \mathrm{tr}(LL^+) = \mathrm{tr}(I_{\mathrm{im}(L)}) = n - 1,
    $$
    we have $M = \sqrt{n - 1}$. So, from Rudelson--Vershynin we get that we need
    $O(n \log n / \eps^2)$ samples.

    \subsection{Algorithms}

    From the existential argument we get an algorithm with running time $O(n^{\omega})$.
    The bottleneck is computing $L^+$.
    But what we actually need is effective resistances:
    $$
        R_{vw} = (e_v - e_w)^t L^+ (e_v - e_w) = \|ML^+ (e_v - e_w)\|_2^2.
    $$
    So, we want to be able to estimate $\ell_2$-distance between the columns of
    $ML^+$. We can use the dimension reduction paradigm: let us multiply $ML^+$ by
    a random $\pm 1$-matrix $A$: so we get $O(\log n) \times n$ matrix $AML^+$ that
    $O(1)$-preserves pairwise $\ell_2$ distances between columns.
    We can compute $AM$ in $\tilde{O}(m)$ time, and our problem reduces to $\tilde{O}(1)$
    computations of the form $L^+ x$ for some vectors $x$. For this we can employ
    Laplacian solvers.

    For any Laplacian $L$ and any $x$ we can find a vector $y$ such that
    $$
        \|y - L^+x\|_L \leq \eps \|L^+x\|_L,
    $$
    where $\|u\|_L := \sqrt{u^t L u}$, in time $\tilde{O}(m \log(1 / \eps))$. It turns out
    that for us it is sufficient to take $\eps = n^{-\Theta(1)}$, so the total running time
    is $\tilde{O}(m)$.

    \subsection{Laplacian Solver}

    \subsection{Sparsifiers with linear number of edges}
    \cite{BSS09}
    }

    \section{Spectral Sparsification}

    \subsection{Preliminaries}

    \subsubsection{Linear Algebra 101}

    Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix. 
    \begin{theorem}[Eigendecomposition]
        All the eigenvalues
        of $A$ are real and there exists an orthonormal basis of $\mathbb{R}^n$
        that consists of $A$'s eigenvectors.

        We can write
        $$
            A = \sum_{i=1}^n \lambda_i u_i u_i^t,
        $$
        where $\lambda_1, \lambda_2, \ldots, \lambda_n$ are the eigenvalues of $A$, and
        $u_1, u_2, \ldots, u_n$ are the corresponding eigenvectors that form an orthonormal
        basis of $\mathbb{R}^n$. This decomposition is called \emph{eigendecomposition}
        of $A$.
    \end{theorem}

    We can use the notion of eigendecomposition to introduce a pseudoinverse of $A$.

    \begin{definition}[Pseudoinverse]
        A \emph{pseudoinverse} of $A$ is the following matrix:
        $$
            A^+ := \sum_{i : \lambda_i \ne 0} \frac{1}{\lambda_i} u_i u_i^t.
        $$
    \end{definition}

    Clearly, if $A$ is non-degenerate (equivalently, $\lambda_i \ne 0$ for every $i$),
    then $A^+ = A^{-1}$.
    On the other hand, for every symmetric $A$
    $$
        A A^+ = A^+ A = I_{\mathrm{im}(A)},
    $$
    where $I_{\mathrm{im}(A)}$ is a linear operator that maps vectors from $\mathrm{ker}(A)$
    to zero and maps vectors from $\mathrm{im}(A)$ to themselves (this definition is correct,
    because we can consider a decomposition
    $\mathbb{R}^n = \mathrm{ker}(A) \oplus \mathrm{im}(A)$).

    If a symmetric matrix is positive semi-definite (equivalently, all the eigenvalues
    are non-negative), then we can consider its square root.

    \begin{definition}[Square Root of a PSD Matrix]
        $$
            A^{1/2} := \sum_{i} \sqrt{\lambda_i} u_i u_i^t.
        $$
    \end{definition}
    Clearly, $A^{1/2} A^{1/2} = A$.

    During the presentation of spectral sparsification algorithms we will use the following
    matrix norm crucially.

    \begin{definition}[Spectral Norm]
        $$
            \|A\|_2 := \max_{x} \frac{\|Ax\|_2}{\|x\|_2}.
        $$
    \end{definition}

    For symmetric matrices spectral norm is just the maximum absolute value of an eigenvalue.
    If $I$ is the identity matrix, then $\|A - I\|_2 \leq \eps$ iff all the eigenvalues of
    $A$ lie within $[1 - \eps; 1 + \eps]$.

    \subsubsection{Graph Laplacian}

    Let $G = (V, E)$ be an undirected graph.

    \begin{definition}[Laplacian]
        The following $n \times n$ matrix $L$ is called the Laplacian of $G$:
        $$
            x^t L x = \sum_{(v,w) \in E} (x_v - x_w)^2. 
        $$
    \end{definition}

    Clearly, $L_{vv}$ equals to the degree of $v$ and $L_{vw} = -1$, if $(v, w) \in E$,
    and $0$, otherwise.
    Since $L$ is positive-semidefinite, all $L$'s eigenvalues are non-negative. Let
    $0 \leq \lambda_1 \leq \lambda_2 \leq \ldots \leq \lambda_n$ be the eigenvalues of $L$.

    \begin{lemma}
        For every graph $G$ $\lambda_1 = 0$. Moreover, $G$ is connected iff $\lambda_2 > 0$.
    \end{lemma}
    \begin{proof}
        Clearly, $\mathbf{1}^t L \mathbf{1} = 0$, so, $\lambda_1 = 0$ and the corresponding
        eigenvector is $\mathbf{1} / \sqrt{n}$.

        If there is a non-zero
        vector within the kernel of $L$ that is orthogonal to $\mathbf{1}$
        it is easy to see that there are no edges connecting negative and positive
        coordinates, so $G$ is disconnected. Conversely, if $G$ is connected and
        $x^t L x = 0$, then, clearly, $x = \alpha \mathbf{1}$.
    \end{proof}

    So, if $G$ is connected, then $\mathrm{ker}(L) = \langle \mathbf{1} \rangle$
    and $\mathrm{im}(L) = \langle \mathbf{1} \rangle^{\perp}$.

    It turns out that $\lambda_2$ determines how well-connected $G$ is. The larger
    $\lambda_2$ is, the better connected $G$ is, and vice versa. This fact is known
    as Cheeger's inequality.

    We can decompose $L$ as follows:
    $$
        L = \sum_{(v,w) \in E} (e_v - e_w) (e_v - e_w)^t.
    $$
    On the other hand,
    \begin{equation}
        \label{full_unity}
        I_{\mathrm{im}(L)} = (L^+)^{1/2} L (L^+)^{1/2} = \sum_{(v,w) \in E} q_{vw} q_{vw}^t,
    \end{equation}
    where $q_{vw} = (L^+)^{1/2} (e_v - e_w)$. Since $q_{vw} \in \mathrm{im}(L)$, we can
    consider (\ref{full_unity}) only in $\mathrm{im}(L)$: it gives us a decomposition of
    $(n-1)\times(n-1)$ identity patrix into the sum of $m$ 1-rank matrices.
    This decomposition is very convenient for both of the algorithms we are about to
    present.

    What does it mean for a weighted graph $G'$ to approximate the Laplacian of $G$ well?
    It means that
    \begin{equation}
        \label{approx_laplacian}
        \sup_x \left|\frac{x^t \tilde{L} x - x^t L x}{x^t L x}\right|\leq \eps,
    \end{equation}
    where $L$ is the Laplacian of $G$, and $\tilde{L}$ is the Laplacian of $G'$
    (technically, we defined the Laplacian only for undirected graphs, but for weighted
    ones the definition remains essentially the same).
    Clearly, it sufficient to check~(\ref{approx_laplacian}) only for
    $x \in \mathrm{im}(L)$.
    If we denote $y = L^{1/2} x$, then we have to check whether
    $$
        \sup_{y \in \mathrm{im}(L)} \left|\frac{y^t (L^+)^{1/2}
            \tilde{L} (L^+)^{1/2} y - y^t y}{y^t y}\right| \leq \eps.
    $$
    But this condition is equivalent to
    \begin{equation}
        \label{closeness}
        \|(L^+)^{1/2} \tilde{L} (L^+)^{1/2} - I_{\mathrm{im}(L)}\|_2 \leq \eps.
    \end{equation}
    If we require $G'$ to be a (weighted) subgraph of $G$, then
    \begin{equation}
        \label{subgraph}
        \tilde{L} = \sum_{(v,w)\in E} m_{vw} (e_v - e_w) (e_v - e_w)^t,
    \end{equation}
    where $m_{vw}$ is a weight of an edge $(v, w)$.

    So, combining~(\ref{full_unity}), (\ref{closeness}) and (\ref{subgraph}),
    we reduce the problem of spectral sparsification to the following clean
    linear-algebraic problem.

    \begin{problem}[Sparsification of Partition of Unity]
        \label{spu}
        Suppose that
        $$
            I = \sum_{i=1}^m q_i q_i^t,
        $$
        where $q_i \in \mathbb{R}^n$
        and $\eps > 0$~--- some parameter.
        What is the minimum $N$ such that there exists a subset $S \subseteq [m]$ with
        $|S| = N$ and non-negative numbers $\alpha_i \geq 0$ for every $i \in [m]$ such that
        $$
            \|I - \sum_{i \in S} \alpha_i q_i q_i^t\|_2 \leq \eps?
        $$
    \end{problem}

    \subsection{Sparsification by Sampling}

    The paper~\cite{SS11} solves Problem~\ref{spu} using random sampling.
    Suppose that we pick $i_1, i_2, \ldots, i_N \in [m]$ independently so that
    $\mathbf{Pr}[i_j = i] = p_i$, where $p_i$ are some probabilities.
    We want to choose $p_i$ and $N$ so that
    $\|I - \sum_{j \in [N]} q_{i_j} q_{i_j}^t / (N\cdot p_{i_j})\|_2 \leq \eps$
    with high (or at least with non-zero) probability (we need the normalizing
    factor $1 / Np_{i_j}$ in order for the random sum to be equal to $I$ in expectation).

    What $N$ and $p_i$ should we choose, if we want $N$ to be relatively small?
    In~\cite{R96,RV07} the following celebrated result was proved.
    \begin{theorem}
        \label{rv_inequality}
        Let $y$ be an $n$-dimensional vector-valued random variable.
        Suppose that $\|y\|_2 \leq M$ with probability $1$ and
        $\|\mathbf{E}[yy^t]\|_2 \leq 1$. Then,
        \begin{equation}
            \label{rv_formula}
            \mathbf{E}[\|\frac{1}{N} \sum_{i = 1}^{N} y_i y_i^t - \mathbf{E}[y_i y_i^t]\|_2]
            \leq O\left(M \sqrt{\frac{\log N}{N}}\right),
        \end{equation}
        where $y_i$ -- independent copies of $y$.
    \end{theorem}

    We apply this theorem to the following random variable: we choose $i \in [m]$
    with probability $p_i$ and then set $y := q_i / \sqrt{p_i}$.
    In order to make the RHS of~(\ref{rv_formula}) as small as possible, we should
    minimize $M = \max_i \|q_i\|_2 / \sqrt{p_i}$. Clearly, in order to do this
    we should choose $p_i$ proportional to $\|q_i\|_2^2$.
    In this case
    $$
        M = \sqrt{\sum_{i=1}^m \|q_i\|_2^2} =
        \sqrt{\sum_{i=1}^m \mathrm{tr}(q_i q_i^t)} =
        \sqrt{\mathrm{tr}\left(\sum_{i=1}^m q_i q_i^t\right)} = \sqrt{n}.
    $$
    So, from Theorem~\ref{rv_inequality} we see that we can take $N = O(n \log n / \eps^2)$
    and get the deviation of the spectral norm at most by $\eps / 2$ in expectation.
    Using Markov inequality we see that with probability at least $1/2$ the deviation
    is at most $\eps$. Repeating the sampling process several times independently
    we can boost the probability of success.

    \subsection{Near-linear Time Laplacian Sparsification}

    The argument from the previous section gives a near-linear time algorithm for solving
    Problem~\ref{spu}. But our initial motivation was to sparsify the Laplacian of a graph
    $G$. The catch here is that our reduction to Problem~\ref{spu} requires a computation
    of $(L^+)^{1/2}$, which we do not know how to do faster than in time $O(n^{2.373})$.
    So, in order to be able to use the argument from the previous section, we have to solve
    the following problem: for every edge $(v, w) \in E$ compute
    $r_{vw} := \|q_{vw}\|_2^2 =
    \|(L^+)^{1/2}(e_v - e_w)\|_2^2 = (e_v - e_w)^t L^+ (e_v - e_w)$.
    Despite we do not know how to compute $r_{vw}$'s exactly in near-linear time we can
    observe, that actually any constant-factor approximations for $r_{vw}$'s are sufficient
    (it will only affect the hidden constant factor in the resulting number of edges).
    Thus, we face the following problem:
    \begin{problem}
        Compute the constant-factor approximations to
        $$
            r_{vw} = (e_v - e_w)^t L^+ (e_v - e_w).
        $$
    \end{problem}

    Let us use the following presentation of $L$: $L = M^t M$, where $M$ is the incidence
    matrix of $G$ ($m$ by $n$ matrix whose rows correspond to the edges, a row that
    corresponds to $(v, w)$ equals to $(e_v - e_w)^t$). Thus,
    $$
        r_{vw} = (e_v - e_w)^t L^+ (e_v - e_w)^t = (e_v - e_w)^t L^+ L L^+ (e_v - e_w) =
        \|M L^+ (e_v - e_w)\|_2^2.
    $$
    So, we are interested in pairwise $\ell_2$-distances between the columns of
    $M L^+$.
    To compute $M L^+$ we can, for example multiply each row of $M$ by $L^+$. It turns
    out that there is a way to do it in near-linear time per row using solvers for graph
    Laplacians (for example, the one described in~\cite{KMP10}).
    But the problem is that we have $m$ rows, so the overall running time would be
    $\tilde{O}(m^2)$.
    Luckily, we can fix this problem. Since we care only about approximate pairwise
    distances we can employ the dimension reduction technique~\cite{DG03}:
    if $Q$ is a random
    $O(\log n) \times m$ matrix whose entries are i.i.d. Gaussians, then with
    high probability the pairwise distances between the columns of $QML^{+}$ approximate
    those from $ML^+$. We can multiply $Q$ by $M$ in time $O(m \log n)$, and then
    invoke the Laplacian solver $O(\log n)$ times.

    \subsection{Iterative Sparsification}

    Unlike~\cite{SS11}, a completely different approach for solving Problem~\ref{spu}
    is used in~\cite{BSS09}.
    If in the RHS of~\ref{rv_formula} there were not $\sqrt{\log N}$ factor, the exactly
    the same sampling procedure would give sparsifiers with $O(n / \eps^2)$ terms.
    Unfortunately, this factor is unavoidable.
    If $y$ is a random basis vector, then, clearly, we need at least $\Omega(n \log n)$
    samples to choose each vector at least once.
    Thus, no matter what probabilities do we use for (independent) sampling, we can not
    overcome the bound $O(n \log n)$.

    So, to get the optimal $O(n / \eps^2)$ one has to use ``deterministic'' methods.
    We will use the iterative approach: on each step we keep track to a current approximation
    $A$ of the identity, and update $A$ by adding a multiple $\alpha q_i q_i^t$ for some
    carefully chosen $\alpha$ and $i$ (this update procedure is called rank-$1$ update).
    Since we want that our resulting matrix to have all eigenvalues within
    $[1 - \eps; 1 + \eps]$, let us first study how eigenvalues behave under rank-$1$
    updates.

    Since the trace of a matrix equals to the sum of its eigenvalues we see that if
    we increase $A$ by $vv^t$, then the sum of eigenvalues increases by $\|v\|_2^2$.
    So, ``on average'' every eigenvalue increases by $\|v\|_2^2 / n$.
    If we managed to get many of such ``uniform'' increases, then we would have been done:
    after some time all eigenvalues will be within $(1 \pm \eps)$ of each other, so after
    the proper normalization they will be within $[1 - \eps; 1 + \eps]$.

    Unfortunately, there is no guarantee that all eigenvalues will be increased by
    (approximately) the same amount.
    Here comes the brilliant idea of Batson, Spielman, and Srivastava~\cite{BSS09}.
    They consider the following ``barrier'' potential functions which allow us to conclude
    that all eigenvalues grow on a steady pace.

    \begin{definition}
        Let use denote by $\lambda_1, \lambda_2, \ldots, \lambda_n$ the eigenvalues of
        a square matrix $A$. Then,
        \begin{eqnarray*}
            L_{x}(A) &:=& \sum_{i} \frac{1}{\lambda_i - x}; \\
            U_{x}(A) &:=& \sum_{i} \frac{1}{x - \lambda_i}.
        \end{eqnarray*}
        Let us call $L_x(A)$ \emph{lower barrier} and $U_x(A)$~--- upper barrier,
        respectively.
    \end{definition}

    Clearly, $L_x(A)$ is increasing in $x$ and $U_x(A)$ is, on the other hand, decreasing.
    These barriers blow up, whenever $x$ equals to some eigenvalue.

    The overall strategy for sparsification will be the following:
    let $\eps_L > 0$, $\eps_U > 0$, $\delta_L > 0$, $\delta_U > 0$, $l_0$, $u_0$ be some
    constants. We have the successive approximations $A_0, A_1, \ldots, A_N$ such that
    \begin{itemize}
        \item $A_0 = 0$;
        \item $A_i = A_{i-1} + \alpha q_j q_j^t$ for some $\alpha \geq 0$ and $j \in [m]$;
        \item $L_{l_0 + i \cdot \delta_L}(A_i) \leq L_{l_0 +
            (i - 1) \cdot \delta_L}(A_{i-1})
              \leq \eps_L$;
        \item $R_{r_0 + i \cdot \delta_R}(A_i) \leq R_{r_0 +
            (i - 1) \cdot \delta_R}(A_{i-1})
              \leq \eps_R$;
        \item all eigenvalues of $A_i$ are within $(l_0 + i \cdot \delta_L; r_0 + i \cdot
              \delta_R)$.
    \end{itemize}

    In the end of the day we will choose the parameters as follows: $\delta_L = 1$,
    $\delta_R = 1 + \Theta(\eps)$, $l_0 = -\Theta(n / \eps)$, $r_0 = \Theta(n / \eps)$.
    Clearly, it implies that after $O(n / \eps^2)$ steps all the eigenvalues will be
    within $(1 \pm \eps)$ of each other, so we are done.

    The main question is, of course, how to choose $\alpha$ and $q_j$ to maintain all the
    invariants. The plan of the argument is the following: first, we derive
    (rather ugly looking) conditions on $j$ that imply that there exists an $\alpha$ such
    that if we set $A_i := A_{i-1} + \alpha q_j q_j^t$, then all the invariants will be
    maintained. Then, we prove that if we choose $j$ uniformly at random, then
    ``on expectation'' these conditions will hold.
    On the algorithmic side it implies that we can try all possible $j$'s and choose the
    one that works.

    So, suppose we want to set $A_i := A_{i-1} + \alpha q_j q_j^t$ for some $\alpha \geq 0$
    and $j \in [m]$. The next Lemma quantifies what $\alpha$ we should choose so that
    $L_{l_0 + i \cdot \delta_L}(A_i) \leq L_{l_0 + (i - 1) \cdot \delta_L}(A_{i-1})$
    and $R_{r_0 + i \cdot \delta_R}(A_i) \leq R_{r_0 + (i - 1) \cdot \delta_R}(A_{i-1})$.

    For compactness let us denote $l_i := l_0 + i \cdot \delta_L$,
    $r_i := r_0 + i \cdot \delta_R$.

    \begin{lemma}
      $L_{l_i}(A_i) \leq L_{l_{i-1}}(A_{i-1})$
      iff
      $$
        0 < \frac{1}{\alpha} \leq -q_j^t (A_{i-1} - l_i I)^{-1} q_j +
        \frac{q_j^t (A_{i-1} - l_i I)^{-2}q_j}{L_{l_i}(A_{i-1}) - L_{l_{i-1}}(A_{i-1})}
        =: \sigma(j).
      $$
      $R_{r_i}(A_i) \leq R_{r_{i-1}}(A_{i-1})$ iff
      $$
        \frac{1}{\alpha} \geq q_j^t (r_i I - A_{i-1})^{-1} q_j +
        \frac{q_j^t (r_i I - A_{i-1})^{-2} q_j}{R_{r_{i-1}}(A_{i-1}) - R_{r_i}(A_{i-1})}
        =: \tau(j). 
      $$
    \end{lemma}
    \begin{proof}
        \footnote{TODO!!!!!}
    \end{proof}

    Argue about eigenvalues!!!!
    
    Now it is left to prove that we can choose the parameters such that on every 
    iteration there exists
    $j \in [m]$ such that $\sigma(j) \geq \tau(j)$.
    We do this using the averaging argument. We show that one choose the parameters such
    that $\sum_j \sigma(j) \geq \sum_j \tau(j)$.

    We have
    \begin{eqnarray*}
        \sum_j \sigma(j) &=& -\sum_t \frac{1}{\lambda_t - l_i} +
        \frac{\sum_t \frac{1}{(\lambda_t - l_i)^2}}
        {\sum_t \frac{1}{\lambda_t - l_i} - \sum_t \frac{1}{\lambda_t - l_{i-1}}};\\ 
        \sum_j \tau(j) &=& \sum_t \frac{1}{r_i - \lambda_t} +
        \frac{\sum_t \frac{1}{(r_i - \lambda_t)^2}}
        {\sum_t \frac{1}{r_{i-1} - \lambda_t} - \sum_t \frac{1}{r_i - \lambda_t}},\\ 
    \end{eqnarray*}
    where $\lambda_t$ are the eigenvalues of $A_{i-1}$.

    One can prove that $\sum_j \sigma(j) \geq 1 / \delta_L - \eps_L$,
    $\sum_j \tau(j) \leq 1 / \delta_R + \eps_R$.

    \section{Vertex Sparsification}
    \bibliographystyle{alpha}
    \bibliography{ir}
\end{document}
